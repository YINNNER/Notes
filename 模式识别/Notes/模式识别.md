[TOC]

# 模式识别

# 2. 聚类分析

## 2.2 相似性测度和聚类准则

### 相似性测度

#### 欧氏距离

> 设$X_1$、$X_2$为两个n维模式样本，$X_1=[x_{11},x_{12},...,x_{1n}]^T, X_2=[x_{21},x_{22},...,x_{2n}]^T$ 
>
> 欧氏距离定义为：
>
> $D(X_1,X_2) = ||X_1-X_2||=\sqrt{(X_1-X_2)^T(X_1-X_2)}=\sqrt{(x_{11}-x_{21})^2+...+(x_{1n}-x_{2n})^2}$
>
> 距离越小，越相似。

注意：



- 各特征维上应当是相同的物理量（x11和x21应该是一样的物理量）
- 注意同类物理量的量纲应该一样



#### 马氏距离

> $D^2=(X-M)^TC^{-1}(X-M)$
>
> X：模式向量；M：均值向量；
>
> C：该类模式总体的协方差矩阵



#### 明氏距离

> $D_m(X_i,X_j)=[\sum^n_{k=1}|x_{ik}-x_{jk}|^m]^{1/m}$
>
> 也c称为m-范数。

- 当m=2时，明氏距离为欧氏距离

- 当m=1时，称为“街坊”距离：

  $D_1(X_i,X_j) = \sum^n_{k=1}|x_{ik}-x_{jk}|$

![1](../../../../../PanNotes/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB.assets/1.PNG)

#### 汉明距离

> 设Xi和Xj为n维二值模式（分量取值1或-1）样本向量，则
>
> $D_h(X_i,X_j)=\frac {1}{2}(n-\sum^n_{k=1}x_{ik}·x_{jk})$

- 该式中的**求和式**，表达的是两个二值向量之间，同值分量（即值相等的分量）与不同值分量数之差，该差值越大，则两个向量越不相似。
- 当这个差值取值为：
  - 0：表示两模式向量的各个分量相同
  - n：表示两模式向量的各个分量都不同
  - n/2：表示两模式分量中，取值相同的分量数比取值不同的分量数多n/2

#### 角度相似性函数

> $S(X_i,X_j)=\frac{X^T_iX_j}{||X_i||·||X_j||}$
>
> 即两向量之间夹角的余弦。



#### Tanimoto测度

> 用于0-1型二值特征情况，定义：
>
> $S(X_i,X_j)=\frac{X_i^TX_j}{X^T_iX_i+X^T_jX_j-X^T_iX_j}=\frac{X_i,X_j之间具有某特征的数量}{X_i,X_j中有某种特征的分量总数}$



### 聚类准则

- 确定聚类准则的两种方式：

  - 阈值准则：根据规定的距离阈值进行判断
  - 函数准则：利用聚类准则函数进行判断

- 聚类准则函数：

  $J=\sum^c_{j=1}\sum_{X\in S_j}||X-M_j||^2$

  - c为聚类类别的数目
  - $M_j=\frac{1}{N_j}\sum_{x\in S_j}X$是属于$S_j$集的样本的均值向量，$N_j$为模式类$S_j$中样本数目

  适用于各类样本密集且数目相差不多，而不同类间的样本又明显分开的情况。

## 2.3 基于距离阈值的聚类算法

### 近邻聚类法

- 算法描述
  1. 任取样本Xi作为第一个聚类中心的初始值Z1
  2. 计算样本X2到Z1的欧氏距离D，若D>T，定义一新的聚类分析Z2=X2，否则X2属于以Zi为中心的聚类
  3. 设已有聚类中心Z1, Z2，计算D31和D32，若D31>T且D32>T，则建立第三个聚类中心
  4. 依次类推

### 最大最小距离算法

- 算法描述
  1. 选任意一模式样本作为第一聚类中心Z1
  2. 选择**离Z1距离最远的样本**作为第二聚类中心Z2
  3. 逐个计算各模式样本Xi与已确定的所有聚类中心Zi之间的距离，并选出其中最小的距离。
  4. 在**所有最小距离中选出最大距离**，如该最大值达到$||Z_1-Z_2||$的一定分数比值以上，则相应的样本点取为心得聚类中心
  5. 重复，直到没有新的聚类中心出现为止。
  6. 将剩余样本按最近距离划分到相应的聚类中心对应的类中

### 层次聚类法（系统聚类法、谱系聚类法）

- 算法描述

  1. N个初始模式样本自成一类，即建立N类：

     $G_1(0),G_2(0),...,G_N(0))$

     计算各类之间的距离，得一N*N维距离矩阵D(0)，0表示初始状态

  2. 假设已球的距离矩阵D(n)，找出D(n)中的最小元素，将其对应的两类合并为一类，由此建立新的分类：

     $G_1(n+1),G_2(n+1),...$

  3. 再次计算：经过合并后，各个类别之间的距离，得D(n+1)

  4. 跳至第2步，重复计算及合并

  5. 取距离阈值T，当D(n)的最小分量超过给定值T时，算法停止，所得即为聚类结果；

     若不取阈值，则直到全部样本聚为一类为止，输出聚类的分级树（得到全部可能的聚类结果——谱系聚类）

- 类间距离计算方法

  - 最短距离法：算出分别属于两个聚类中两个样本的欧氏距离，比较所有的欧氏距离，以最小的距离作为类间距离

  - 最长距离法：类比上

  - 中间距离法：若K类由I类和J类合并而成，则H和K类之间的距离为：

    $D_{HK}=\sqrt{\frac{1}{2}D^2_{HI}+\frac{1}{2}D^2_{HJ}-\frac{1}{4}D^2_{IJ}}$

  - 重心法

    $D_{HK}=\sqrt{\frac{n_I}{n_I+n_J}D^2_{HI}+\frac{n_J}{n_I+n_J}D^2_{HJ}-\frac{n_In_J}{(n_I+n_J)^2}D^2_{IJ}}$

  - 类平均距离法

    $D_{HK}=\sqrt{\frac{1}{n_Hn_K}\sum_{i\in H,j\in K}d^2_{ij}}$

    $d^2_{ij}$：H类任一样本Xi和K类任一样本Xj之间的欧氏距离平方



## 2.5 动态聚类法

### K-均值算法

- 算法描述

  1. 任选K个初始聚类中心：$Z_1(1),Z_2(1),...,Z_K(1)$（括号内为迭代的次序号）

  2. 按最小距离原则将其余样本分配到K个聚类中心中的一个

  3. 再次计算各个聚类中心的新向量值$Z_j(k+1)$

  4. 如果$Z_j(k+1) \ne Z_j(k)$，则回到2.，将模式样本逐个重新分类，重复迭代计算。

     如果$Z_j(k+1) = Z_j(k)$，算法收敛，计算完成。

- 根据$J_j-K$关系曲线，曲线拐点对应着接近最优的K值





# 3. 判别函数分类法

## 3.1 判别函数

- 统计模式识别：按**任务类型**划分：
  - 聚类分析
  - 判别分析
    - 几何分类法
    - 概率分类法
    - 近邻分类法
- 判别函数定义：直接用来对模式进行分类的决策函数
- 判别函数正负值的确定：是在训练判别函数的权值时**人为确定的**

## 3.2 线性判别函数

- 一般形式：

  $d(X)=w_1x_1+w_2x_2+...+w_nx_n+w_{n+1}=W^T_0X+w_{n+1}$

  式中：$X=[x_1,x_2,...,x_n]^T$，$W_0=[w_1,w_2,...,w_n]^T$：权向量，即参数向量。

- 增广向量形式：

  $d(X)=w_1x_1+w_2x_2+...+w_nx_n+w_{n+1}·1=[w_1 w_2 ... w_n w_{n+1}]\left[\begin{matrix} x_1 \\ x_2 \\...\\x_n\\1 \end{matrix}\right]=W^TX$

### 3.2.2 线性判别函数的性质

1. 两类情况

$$
d(X)=W^TX \left\{
\begin{aligned}
>0,若X\in w_1 \\
<0,若X\in w_2
\end{aligned}
\right.
$$

1. 多类情况

   - 对M个线性可分模式类，$w_1,w_2,...,w_M$，有三种分类方式：

     - $w_i/\overline{w_i}$，是非两分法
     - $w_i/w_j$，成对两分法
     - $w_i/w_j$，成对两分法特例（没有不确定区）

   - **是非两分法**
     $$
     d_i(X)=W^T_iX\left\{
     \begin{aligned}
     >0, 若X\in w_i \\
     <0, 若X\in \overline{w_i}
     \end{aligned}
     \right.   i=1,..., M
     $$

     - 将某个待分类模式X 分别代入M 个类的d (X)中，
       **若只有$d_i(X)>0$，其他$d_j(X)$均<0，则判为$w_i$类**
     - ![3](images/3.PNG)

   - **成对两分法**

     - 一个判别界面只能分开两个类别，不需要把其余所有的类别都分开。

     - 判决函数：$d_{ij}(X)=W^T_{ij}X$，这里$d_{ji}=-d_{ij}$

       $d_{ij}(X)>0, \forall j\ne i; i,j =1,2,...,M, 若X\in w_i$

     - 在M类模式中，**以i开始的**M-1个判决函数全为正时，$X\in w_i$，其中若有一个为负，则为IR区

     - 能用本方法分类的模式集称为：**成对线性可分的**

     - M类共需要$C^2_M=\frac{M(M-1)}{2!}$ 个判决函数

   - **成对两分法特例（没有不确定区）**

     - 若成对两分法中的判别函数，可以分解为
       $$
       d_{ij}(X)=d_i(X)-d_j(X)
       $$
       时，那么$d_i(X)>d_j(X)$就相当于成对两分法中的$d_{ij}(X)>0$

       此时，对M类情况，等价的判别函数性质为：

       $d_i(X)>d_j(X), \forall j\ne i; i,j =1,2,...,M, 若X\in w_i$

     - 能用本方法分类的模式集称为：**线性可分的**

     - 若在第三种情况下可分，则在第二种情况下也可分，但反之不一定成立

     - 除边界区外，**没有不确定区域**

   - $w_i/\overline{w_i}（是非两分法）与w_i/w_j（成对两分法）$两分法的比较：

     - $w_i/\overline{w_i}$两分法共需要**M**个判别函数，$w_i/w_j$需要M(M-1)/2个
     - 当M>3时，后者需要更多的判别式，但对模式集进行线性可分的可能性要更大一些
       - 一种类别模式$w_j$的分布要比M-1类模式$\overline{w_i}$的分布更为聚集，因
         此$w_i/w_j$两分法受到的限制比$w_i/\overline{w_i}$少，因此线性可分可能性大。

## 3.3 广义线性判别函数

- 广义线性判别的目的：

  - 通过某映射，把模式空间X编程X\*，以便将X空间中非线性可分的模式集变成在X\*空间中的线性可分的模式集

- 非线性多项式判别函数

  - 设一训练用模式集，{X}在模式空间X中线性不可分，非线性多项式判别函数形式如下：

    $d(X)=w_1f_1(X)+w_2f_2(X)+...+w_kf_k(X)+w_{k+1}=\sum^{k+1}_{i=1}w_if_i(X)$

    - $f_i(X)$取什么形式以及d(X)取多少项，取决于非线性边界的复杂程度

- 广义线性函数的模式向量定义为：

  $X^*=[x_1^*,x_2^*,...,x_k^*,1]^T=[f_1(X),f_2(X),...,f_k(X),1]^T$

- 存在的问题：

  - 非线性变换可能非常复杂
  - 维数大大增加，导致维数灾难

## 3.4 线性判别函数的几何性质

### 3.4.1 模式空间与超平面

概念：

- **模式空间**：以n维模式向量X的n个分量为坐标变量的欧氏空间。

- **模式向量**：点、有向线段。

- **线性分类**：用d(X)进行分类，相当于用超平面d(X)=0把模式空
  间分成不同的决策区域。

- **超平面的法向量**：

  - X1,X2在超平面上：

  ![4](images/4.PNG)

  ​	$W_0^T(X_1-X_2)=0$，W0即超平面d(X)的法向量

  ​	**超平面的单位法线向量为U：**

  ​	$U=\frac{W_0}{||W_0||}$，$||W_0||=\sqrt{w_1^2+w_2^2+...+w_n^2}$

  - X不在超平面上：

    ![5](images/5.PNG)

    **判别函数d(X)正比于点X到超平面的代数距离**

    $r=\frac{d(X)}{||W_0||}$

  - X在原点

    $r_0=\frac{w_{n+1}}{||W_0||}$

    **原点在超平面的正负侧位置由阈值权$w_{n+1}$决定**

### 3.4.2 权空间与权向量解

- 概念：

  - **权空间**：以$d(X)=w_1x_1+w_2x_2+...+w_nx_n+w_{n+1}$的权系数为坐标变量的（n+1）维欧氏空间，X为已知。
  - **增广权向量**：点、有向线段

- 线性分类

  设增广样本向量：
  $$
  w_1类：X_{11},X_{12},...,X_{1p} \\
  w_2类：X_{21},X_{22},...,X_{2q}
  $$
  使d(X)将w1和w2分开，需满足：
  $$
  d(X_{1i})>0,i=1,2,...,p \\
  d(X_{2i})<0,i=1,2,...,q
  $$
  g给w2的q个增广模式乘-1，统一为：
  $$
  d(X)>0,其中X=\left\{
  \begin{aligned}
  X_{1i},i=1,2,...,p \\
  -X_{2i},i=1,2,...,q
  \end{aligned}
  \right.
  $$
  这就是**样本的符号规范化**，X：**规范化增广样本向量**



### 3.4.3 线性二分能力

- 是指线性函数对给定的N个n维二类模式的全部可能的类别分布情况，能正确分类
  的情况数。

- 若限定用线性判别函数，且样本在模式空间是**良好分布的**，即在n维模式空间中没有(n+1)个模式位于(n－1)维子空间中，可以证明：N个n维二类模式集用线性判别函数能正确分类的方法总
  数，称为：该模式集的**线性二分总数：**
  $$
  D(N,n)=
  \left\{
  \begin{aligned}
  & 2\sum^n_{j=0}C^j_{N-1}=2(C^0_{N-1}+C^1_{N-1}+...+C^n_{N-1})， & N>n+1 \\
  & 2^N=2(C^0_{N-1}+C^1_{N-1}+...+C^{N-1}_{N-1}), & N\le n+1
  \end{aligned}
  \right.
  $$
  或该模式集的**线性二分概率**：
  $$
  P(N,n)=\frac{D(N,n)}{2^N}=
  \left\{
  \begin{aligned}
  & 2^{1-N}\sum^n_{j=0}C^j_{N-1}, &N>n+1 \\
  & 1, & N\le n+1
  \end{aligned}
  \right.
  $$






- **只要模式集中模式个数N小于或等于增广模式的维数（n+1），模式类总是线性可分的。**

- 若定义$\lambda = N/(n+1)$，可观察到：

  - 在$\lambda \le 1$时，也即在$N\le (n+1)$时，有P(N,n)=1.

    即：这样的N个模式是线性可分的。

  - 在$1\le \lambda \le 2$时，也即$(n+1)\le N\le 2(n+1)$时，$0.5\le P(N,n)\le 1$

    即：当N固定、n增加时，模式集的线性二分概率是增加的。

  - 在$\lambda >2$时，也即在$N\ge (n+1)$时，$P(N,n)<0.5$

    即：当n固定、N增加时，模式集的线性二分概率迅速下降。

  将$\lambda=2$时的N值定义为阈值$N_0$，称为**二分法能力（容量）**，即：
  $$
  N_0=2(n+1)
  $$






## 3.6 感知器算法

- 用以利用已知类别的模式样本训练权向量W。

#### 感知器算法：

$$
两类线性可分的模式类：\omega _1,\omega _2 ,设d(X)=W^TX, \\
其中，W=[w_1,w_2,w_3,...,w_4]^T,X=[x_1,x_2,...,x_n,1]^T \\
应具有性质d(X)=W^TX
\left\{
\begin{aligned}
& >0, &若X\in \omega_1 \\
& <0， &若X\in \omega_2
\end{aligned}
\right.
$$

- 规范化处理：即$\omega_2$类样本全部乘以-1，则有：
  $$
  d(X)=W^TX>0
  $$




- 感知器算法的基本思想：用训练模式验证当前权向量的合理性，如果不合理，就根据误差进行反向纠正，直到全部训练样本都被合理分类。本质上是**梯度下项方法类**。

- 算法步骤：

  - 选择N个分属于$w_1$和$w_2$类的模式样本构成训练样本集$\{X_1,...,X_N\}$

    构成增广向量形式，并进行规范化处理。任取权向量初始值W(1)，开始迭代。初始迭代次数k=1

  - 用全部训练样本进行一轮迭代，计算$W^T(k)X_i$的值，并修正权向量。

    - 若$W^T(k)X_i\le 0$，说明分类器对第i个模式做了错误分类，校正为：
      $$
      W(k+1)=W(k)+cX_i
      $$

      - c：正的校正增量（步长）

    - 若$W^T(k)X_i>0$，分类正确，权向量不变：$W(k+1)=W(k)$

      统一写为：
      $$
      W(k+1)=
      \left\{
      \begin{aligned}
      &W(k),&若W^T(k)X_i>0 \\
      &W(k)+cX_i, &若W^T(k)X_i\le 0
      \end{aligned}
      \right.
      $$

  - 只要有一个错误分类，回到上一步，直到对所有样本正确分类。

- **感知器算法是一种赏罚过程。**

#### 感知器算法用于多类情况

- 对多类情况3，应有：

  > 若$X\in \omega_i，则d_i(X)>d_j(X), \forall j\ne i, j=1,...,M$， 对于M类模式应存在M个判决函数：{$d_i,i=1,...,M$}

- 算法主要内容：

  - 训练样本为增广向量模式，但**不需要符号规范化处理**。

  - 第k次迭代时，计算所有判别函数
    $$
    d_j(k)=W^T_j(k)X,j=1,...,M
    $$
    分两种情况修改权向量：

    1. 若$d_i(k)>d_j(k)，\forall j\ne i,j=1,2,...,M$，则权向量不变，$W_j(k+1)=W_j(k),j=1,2,...,M$

    2. 若第l个权向量使$d_i(k)\le d_l(k)$，则相应的权向量做调整：
       $$
       \left\{
       \begin{aligned}
       &W_i(k+1)=W_i(k)+cX \\
       &W_l(k+1)=W_l(k)-cX \\
       &W_j(k+1)=W_j(k),j\ne i,l
       \end{aligned}
       \right.
       ,c为正的校正增量
       $$






## 3.7 梯度算法

### 3.7.1 梯度算法基本原理

#### 梯度概念

设函数f(Y)是向量$Y=[y_1,y_2,...,y_n]^T$的函数，则f(Y)的梯度定义为：
$$
\nabla f(Y)=\frac{d}{dY}f(Y)=[\frac{\partial f}{\partial y_1},\frac{\partial f}{\partial y_2},...,\frac{\partial f}{\partial y_n}]^T
$$

- 重要性质：**指出函数f在其自变量增加时，增长最快的方向**
  - 即：梯度的**方向**是函数$f(Y)$在Y点增长最快的方向
  - 梯度的**模**是$f(Y)$在增长最快的方向上的增长率（增长率最大值）
  - 所以**负梯度指出了最陡下降方向**

#### 梯度算法

设两个线性可分的模式类$w_i$和$w_2$的样本共N个，$w_2$类样本乘（-1），将两类样本分开的判决函数d(X)应满足：
$$
d(X_i)W^TX_i>0, i=1,2,...,N
$$
思想：**将联立不等式求解W的问题，转换成求准则函数极小值的问题。**

##### 基本思路

定义一个对错误分类敏感的准则函数$J(W,X)$，在J的梯度方向上对权向量进行修改，一般关系表示成从$W(k)$导出$W(k+1)$：
$$
W(k+1)=W(k)+c(-\nabla J)=W(k)-c\nabla J \\
W(k+1)=W(k)-c[\frac{\partial J(W,x)}{\partial W}]_{W=W(k)}
$$
其中c是正的比例因子。

##### 求解步骤

1. 规范化处理，选择准则函数，设置初始权向量W(1)，括号内为迭代次数k=1

2. 依次输入训练样本X，设第k次迭代时输入样本为$X_i$，此时以有权向量$W(k)$，求$\nabla J(k)$：
   $$
   \nabla J(k)=\frac{\partial J(W,X_i)}{\partial W}|_{W=W(k)}
   $$
   权向量修正为：
   $$
   W(k+1)=W(k)-c\nabla J(k)
   $$
   迭代次数加1，重复该步骤，直至对全部训练样本完成一轮迭代。

3. 在一轮迭代中，如果有一个样本使$\nabla J\ne 0$，会到2. 进行下一轮迭代，否则，W不再变化，算法收敛。

### 3.7.2 固定增量法

准则函数：**$J(W,X)=\frac{1}{2}(|W^TX|-W^TX)$**

当$W^TX>0$时，该准则函数有唯一最小值0

**固定增量算法：**
$$
\nabla J=\frac{\partial J(W,X)}{\part W}=\frac{1}{2}[Xsgn(W^TX)-X], \\
其中，sgn(W^TX)=\left\{
\begin{aligned}
& +1, &若W^TX>0 \\
&-1, &若W^TX\le 0
\end{aligned}
\right.
$$

$$
W(k+1)=W(k)+\left\{
\begin{aligned}
& 0,&若W^T(k)X>0 \\
& cX, &若W^T(k)X\le 0
\end{aligned}
\right.
$$

**感知器算法是梯度法的特例。**



## 3.8 最小平方误差算法（最小均方误差算法）

- LMS算法，亦称为Ho-Kashyap算法
- 感知器算法、梯度算法、固定增量算法或其他类似方法，只有当模式类可分离时才收敛，不可分情况下，算法会来回摆动，始终不收敛。
- LMS算法特点：
  - 对可分模式收敛
  - 对类别不可分的情况也能指出来

### 分类器的不等式方程

如果给出分属于$\omega_1,\omega_2$两个模式类的训练样本集$\{X_i,i=1,2,...,N\}$，应满足：$W^TX_i>0$，其中，$X_i$是规范化增广样本向量，$X_i=[x_{i1},x_{i2},...,x_{in},1]^T$

写成矩阵：
$$
\begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1n} & 1 \\
x_{21} & x_{22} & \cdots & x_{2n} & 1 \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
-x_{N1} & -x_{N2} & ... & x_{Nn} & 1 \\
\end{bmatrix}_{N*(n+1)}
\begin{bmatrix}
w_1 \\
w_2 \\
\vdots \\
w_n \\
w_{n+1}
\end{bmatrix}_{(n+1)*1}>
\begin{bmatrix}
0 \\
0 \\
\vdots \\
0 \\
0 \\
\end{bmatrix}_{N*1}=0
$$
即$XW>0$

### LMS算法

- 原理：把对满足$XW>0$的求解，改为满足$XW=B$的求解。

  ​	   其中B为各分量均为正值的矢量。

  - 因为一般训练样本数N总是大于模式的维数n，所以为矛盾方程组，无解，只能求近似解，即
    $$
    ||XW^*-B||=极小
    $$

  - **LMS出发点**：选择一个准则函数，使得当J达到最小值时，XW=B可得到近似解（**最小二乘近似解**）

- 准则函数定义为
  $$
  J(W,X,B)=\frac{1}{2}||XW-B||^2
  $$

  - 最小二乘：
    - 最小：使方程组两边误差最小，即J最小
    - 二乘：二次方
    - 即**最小平方（误差算法）**

- LMS算法核心：通过准则函数极小找W、B

- 算法递推公式：
  $$
  B(k+1)=B(k)+c[e(k)+|e(k)|] \\
  W(k+1)=W(k)+cX^\#|e(k)|\quad其中，X^\#=(X^TX)^{-1}X^T
  $$


#### 模式类别可分性判别

- **当模式集线性可分，且校正系数c满足$0<c\le 1$时，该算法收敛，可求得解W。**
- 通常每次迭代计算后都检查一下**XW(k)**和误差向量**e(k)**，从而可以判断是否收敛：

  分以下几种情况：
$$
  \begin{aligned}
  &①\ 如果e(k)=0，表明XW(k)=B(k)>0，有解。 \\
  &②\ 如果e(k)>0，表明XW(k)>B(k)>0，隐含有解，继续迭代，
  可使e(k)\rightarrow0.\\
  &③\ 如果e(k)<0（所有分量为负数或零，但不全为零），停止迭代，无解。
  \end{aligned}
$$
  综上所述：**只有当e(k)中有大于零的分量（说明：含有纠正信息）时，才需要继续迭代。**一旦e(k)的全部分量只有0和负数，则立即停止。

#### 算法描述

1. 根据N个分属于两类的样本，写出规范化增广样本矩阵X
2. 求X的伪逆矩阵$X^\#=(X^TX)^{-1}X^T$
3. 设置初值c和B(1)，c为正的校正增量，B(1)的各分量大于零，迭代次数k=1.开始迭代
4. 计算e(k)=XW(k)-B(k)，进行可分性判别。
5. 计算W(k+1)和B(k+1)，迭代次数加1，返回4.



## 3.9 非线性判别函数

- 用于线性不可分情况。如分段线性、超曲面。

### 3.9.1 分段线性判别函数

#### 一般分段线性判别函数

设有M类模式，将$\omega_i$类划分为$l_i$个子类：

$\omega_i:\{\omega_i^1,\omega_i^2,\omega_i^3,...,\omega_i^{l_i}\}\quad i=1,2,...,M$

其中第n个子类的判别函数：

$d_i^n(X)=(W_i^n)^TX \quad n=1,2,...,l_i;\ i=1,2,...,M$

$\omega_i$类的判别函数定义为：

$d_i(X)=max\{d_i^n(X),\quad n=1,2,...,l_i\}$

M类的判决规则：

$若d_j(X)=max\{d_i(X),\quad i=1,2,...,M\}，则X\in \omega_j$

#### 基于距离的分段线性判别函数

##### 最小距离分类器

$$
\begin{split}

&设\omega_1类均值向量：M_1=\frac{1}{N_1}\sum^{N_1}_{i=1}X_i \\
&w_2类均值向量：M_2=\frac{1}{N_2}\sum^{N_2}_{i=1}X_i
\\ \\ 
&任一模式X到M_1和M_2的欧氏距离平方：\\
&d_1(X)=||X-M_1||^2 \quad d_2(X)=||X-M_2||^2  \\
\\
&判决规则：若d_1(X)<d_2(X),则X\in \omega_1 \\
&若d_2(X)<d_1(X)，则X\in \omega_2 \\
&判别界面方程： \\
&||X-M_1||^2=||X-M_2||^2 \\
&化简得：2(M_1-M_2)^TX+(M_2^TM_2-M_1^TM_1)=0
\end{split}
$$

##### 分段线性距离分类器

$$
\begin{split}
& 设：M类模式，其中\omega_i类划分为l_i个子类，第n个子类的均值向量为M_i^n。每个子类的判别函数：\\
&\quad \quad d_i^n(X)=||X-M_i^n||^2,n=1,2,...,l_i;i=1,2,...,M \\
&每个类的判别函数：\\
&\quad\quad d_i(X)=min\{d_i^n(X),n=1,2,...,l_i\},i=1,2,...,M \\
&判决规则：\\
&\quad\quad若d_j(X)=min\{d_i(X),i=1,2,...,M\}，则X\in \omega_j
\end{split}
$$

### 3.9.2 分段线性判别函数的学习方法

1. 已知子类划分时的学习方法：
   - 每个子类看成独立的类
   - 在一类范围内根据多累情况3，学习各子类判别函数
   - 继而得到各类判别函数
2. 已知子类数目时的学习方法
   - 用类似于固定增量算法的错误修正算法学习分段线性判别函数。
3. 未知子类数目时的学习方法
   - 树状分段线性分类器

### 3.9.3 势函数法

#### 概念

- 每个点比拟为点能源，在点上势能达到峰值，随着与该点距离的增大，势能分布迅速减小。

  - $\omega_1$类样本势能为正——势能积累形成“高地”

  - $\omega_2$类样本势能*(-1)——势能积累形成“凹地”

    在两类电势分布之间，选择合适的等势面（如零等势面），即可认为是判别界面。

#### 势函数法判别函数的产生

设初始基类势函数$K_0(X)=0$，下标为迭代次数。

步骤：

1. 加入训练样本X1,
   $$
   K_1(X)=
   \left\{
   \begin{align}
   K_0(X)+K(X,X_1),若X_1\in \omega_1 \\
   K_0(X)-K(X,X_1),若X_1\in \omega_2
   \end{align}
   \right.
   $$
   $K_1(X)$描述了加入第一个样本后的边界划分。

2. 加第二个训练样本X2，分三种情况：
   $$
   \begin{align}
   (1)&若X_2\in\omega_1且K_1(X_2)>0,或X_2\in\omega_2且K_1(X_2)<0，\\
   &分类正确，势函数不变：K_2(X)=K_1(X) \\
   (2)&若X_2\in\omega_1但K_1(X_2)\le 0,错误分类，修改势函数：\\
   &K_2(X)=K_1(X)+K(X,X_2)=\pm K(X,X_1)+K(X,X_2) \\
   (3)&若X_2\in\omega_2但K_1(X_2)\ge0，错误分类，修改势函数： \\
   &K_2(X)=K_1(X)-K(X,X_2)=\pm K(X,X_1)-K(X,X_2)
   \end{align}
   $$
   .......

   以上决定积累位势的迭代算法可写成：
   $$
   K_{k+1}(X)=K_k(X)+r_{k+1}K(X,X_{k+1}) 
   $$
   其中$r_{k+1}$为校正项系数，定义为：
   $$
   r_{k+1}=
   \left\{
   \begin{align}
   0,\quad &对于X_{k+1}\in\omega_1且K_k(X_{k+1})>0 \\
   0,\quad &对于X_{k+1}\in\omega_2且K_k(X_{k+1})<0 \\
   1,\quad &对于X_{k+1}\in\omega_1且K_k(X_{k+1})\le0 \\
   -1,\quad &对于X_{k+1}\in\omega_2且K_k(X_{k+1})\ge0 \\
   \end{align}
   \right.
   $$
   略去不使积累势发生变化的样本，可得一简化样本序列$\{\widehat{X_1},\widehat{X_2},...,\widehat{X_j},...\}$，算法可归纳为：
   $$
   K_{k+1}(X)=\sum_{X_j}\alpha_jK(X,\widehat{X_j}),式中 \\
   \alpha_j=\left\{
   \begin{align}
   1,\quad &对于\widehat{X_j}\in\omega_1 \\
   -1,\quad&对于\widehat{X_j}\in\omega_2 
   \end{align}
   \right.
   $$
   即：由(k+1)个训练样本产生的积累势，等于两类中校正错误的样本的总势能之差。



#### 势函数的选择

- 势函数应具备的条件：

  两个n维向量$X$和$X_k$的函数$K(X,X_k)$，如同时满足下列三个条件，都可作为势函数：

  - $K(X,X_k)=K(X_k,X)$，当且仅当$X=X_k$时达到最大值
  - 当向量$X$与$X_k$的距离趋于无穷时，$K(X,X_k)$时趋于零
  - $K(X,X_k)$是光滑函数，且是$X$与$X_k$之间距离的单调下降函数

- 构成势函数的方法：

  - **Ⅰ型势函数**：用对称的有限项多项式展开，即：
    $$
    K(X,X_k)=\sum^m_{i=1}\varphi_i(X_k)\varphi_i(X)
    $$
    式中$\{ \varphi_i (X),i=1,2,...\}$，在模式定义域内应为正交函数集（即内积=0，内积：$(y,z)=\int^b_ay(x)z(x)d(x)$）

    判别函数：
    $$
    d_{k+1}(X)=d_k(X)+\sum^m_{i=1}r_{k+1}\varphi_i(X_{k+1})\varphi_i(X)
    $$

  - **Ⅱ型势函数**：直接选择双变量$X$和$X_k$对称函数作为势函数，即$K(X,X_k)=K(X_k,X)$ 

# 4. 统计决策分类法

## 4.1 基础

**贝叶斯公式**：
$$
P(A_i|B)=\frac{P(A_i)P(B|A_i)}{\sum^n_{i=1}P(A_i)P(B|A_i)}
$$
**先验概率**：即$P(A_i)$，即未获得观察结果前，对原因Ai在未来产生结果的概率的认定

**后验概率**：即$P(A_i|B)$，获得观察结果B之后，结合先验知识，计算出的对结果B是由原因Ai引起的概率。

**贝叶斯公式的密度函数形式**：
$$
\pi(\theta|x)=\frac{h(x,\theta)}{m(x)}=\frac{p(x|\theta)\pi(\theta)}{\int p(x|\theta)\pi(\theta)d\theta}
$$

## 4.2 贝叶斯决策

### 4.2.1 最小错误率贝叶斯决策

#### 决策规则

> 设有M类模式，若$P(\omega_i|X)=max{P(\omega_i|X)},\ j=1,2,...,M$，则$X\in \omega_i$类

- $P(\omega_i|X)$可用贝叶斯公式求得

##### 对两类问题

- 若$p(X|\omega_1)P(\omega_1)>p(X|\omega_2)P(\omega_2)$，则$X\in\omega_1$
- 若$p(X|\omega_1)P(\omega_1)<p(X|\omega_2)P(\omega_2)$，则$X\in\omega_2$

即：
$$
l_{12}(X)=\frac{p(X|\omega_1)}{p(X|\omega_2)} \begin{matrix}
> \\
<
\end{matrix}
\frac{P(\omega_2)}{P(\omega_1)}，则X\in 
\left\{
\begin{aligned}
\omega_1 \\
\omega_2
\end{aligned}
\right.
$$
对数形式：
$$
h(X)=lnl_{12}(X)=lnp(X|\omega_1)-lnp(X|\omega_2)
\begin{matrix}
> \\
<
\end{matrix}
ln\frac{P(\omega_2)}{P(\omega_1)},\ 则X\in 
\left\{
\begin{aligned}
\omega_1 \\
\omega_2
\end{aligned}
\right.
$$

- $l_{12}$称为**似然比**，$\frac{P(\omega_2)}{P(\omega_1)}$为**似然比阙值**

### 4.2.2 最小风险贝叶斯决策

**基本思想**：

以各种错误分类所造成的条件风险（后验风险）最小为规则，进行分类决策。

#### 决策规则

$$
r_i 为X被判决为每一类的条件均风险，则决策规则： \\
若r_k(X)=min\{r_i(X),\ i=1,...,M\}\ \ 则X\in \omega_k
$$

其中：
$$
r_i(X)=\sum^M_{j=1}L_{ij}(X)P(\omega_j|X)
$$

- i：分类判决后指定的类号
- j：样本实际属于的类别号
- $L_{ij}$：将自然属性是$\omega_j$类的样本决策为$\omega_i$类时的是非代价，即**损失函数**

##### 多类情况

设有M类，对于任一X对应M个条件风险：
$$
r_i(X)=\sum^M_{j=1}L_{ij}(X)P(\omega_j|X), \ i=1,2,...,M
$$
决策规则：
$$
若r_k(X)<r_i(X), \ i=1,2,...,M;\ i\ne k,则X\in \omega_k
$$

##### 两类情况

$$
\begin{aligned}
& 当样本X被判为\omega_1 类时：\\
& \quad r_1(X)=L_{11}p(X|\omega_1)P(\omega_1)+L_{12}p(X|\omega_2)P(\omega_2) \\
& 当X被判为\omega_2类时：\\
& \quad r_2(X)=L_{21}p(X|\omega_1)P(\omega_1)+L_{22}p(X|\omega_2)P(\omega_2) \\
& 决策规则：\\
& \quad 若r_1(X)<r_2(X) \quad 则X\in \omega_1 \\
& \quad 若r_1(X)>r_2(X) \quad 则X\in \omega_2
\end{aligned}
$$

定义：
$$
l_{12}(X)=\frac{p(X|\omega_1)}{p(X|\omega_2)}，称为似然比 \\
\theta_{12}=\frac{(L_{12}-L_{22})P(\omega_2)}{(L_{21}-L_{11})P(\omega_1)}，称为阈值
$$
**判别步骤：**

1. 定义损失函数$L_{ij}$

2. 计算似然比阈值$\theta_{12}$

3. 计算似然比$l_{12}(X)$

4. 若$l_{12}(X)>\theta_{12}$，则$X\in \omega_1$

   若$l_{12}(X)<\theta_{12}$，则$X\in \omega_2$

   若$l_{12}(X)=\theta_{12}$，任意判决



#### （0-1）损失最小风险贝叶斯决策

即损失函数为：$L_{ii}=0,\ \ L_{ij} = 1,\ i\ne j$

##### 多类情况

判决函数的等价形式：$d_i(X)=p(X|\omega_i)P(\omega_i),i=1,2,...,M$

决策规则的等价形式：
$$
若d_k(X)>d_i(X),i=1,2,...,M,i\ne k \quad ,则X\in \omega_k
$$

##### 两类情况

以多类情况类推



### 4.2.3 正态分布模式的贝叶斯决策

#### 正态分布的最小错误率贝叶斯决策规则

##### 多类情况

具有**M**种模式类别的**多变量**正态密度函数为：
$$
p(X|\omega_i)=\frac{1}{(2\pi)^{n/2}|C_i|^{1/2}}exp\{-\frac{1}{2}(X-M_i)^TC_i^{-1}(X-M_i)\}, \quad i=1,2,...,M
$$
定义：

均值向量：$M_i=E_i[X]$（表明了区域中心的位置）

协方差矩阵：$C_i=E_i[(X-M_i)(X-M_i)^T]$（反映样本分布区域的形状）

**判决则带入最小错误率贝叶斯决策的判别函数中**

##### 两类问题

判别函数：
$$
d_i(X)=lnP(\omega_i)-\frac{1}{2}ln|C_i|-\frac{1}{2}\{(X-M_i)^TC^{-1}_i(X-M_i)\}
$$
决策规则：
$$
若d_1(X)-d_2(X)
\left\{
\begin{aligned}
>0,\quad 则X\in\omega_1 \\
<0,\quad 则X\in \omega_2
\end{aligned}
\right.
$$

## 4.3 贝叶斯分类器的错误率

### 4.3.1 概念

- 错误率：将应属于某一类的模式错分到其他类中的概率。是衡量分类器性能优劣的重要参数。
  $$
  P(e)=\int^\infty_{-\infty}P(e|X)p(X)dX \\
  其中，X=[x_1,...,x_n]^T,P(e|X)是X的条件错误概率
  $$

- 错误率计算或估计方法：

  - 按理论公式计算
  - 计算错误率上界
  - 实验估计

### 4.3.2 错误率分析

#### 两类问题的错误率

有两种错误：

1. 将来自w1类的模式错分到R2中去
2. 将来自w2类的模式错分到R1中去

错误率为两种错误之和：
$$
P(e)=\int_{R_2}P_1(e|X)p(X)dX+\int_{R_1}P_2(e|X)p(X)dX
$$

##### 最小错误率贝叶斯决策规则

用后验概率密度表示为
$$
\left\{
\begin{aligned}
&若\quad P(\omega_1|X)>P(\omega_2|X),\quad 则X\in \omega_1 \\
&若\quad P(\omega_1|X)<P(\omega_2|X),\quad 则X\in \omega_2
\end{aligned}
\right.
$$
判别界面为：
$$
P(\omega_1|X)=P(\omega_2|X) \\
或\quad p(X|\omega_1)P(\omega_1)=p(X|\omega_2)P(\omega_2)
$$

#### 多类情况错误率

总错误率
$$
P(e)=\sum^M_{i=1}\sum^M_{j=1}\int_{R_i}p(X|\omega_j)P(\omega_j)dX
$$
正确分类概率
$$
P(c)=\sum^M_{i=1}\int_{R_i}p(X|\omega_i)P(\omega_i)dX
$$

### 4.3.3 正态分布贝叶斯决策的错误率计算

#### 正态分布的对数似然比

（第5讲 待继续）



## 4.4 耐曼-皮尔逊决策

- 基本思想：限制某一类错误率为一个确定值，求取使得另一类错误率最小的判决规则与判决阈值

- 两类错误率：
  $$
  P_1(e)=\int_{R_2}p(X|\omega_1)dX \;\; \omega_1类模式被误判为\omega_2类的错误率\\
  P_2(e)=\int_{R_1}p(X|\omega_2)dX \;\; \omega_2类模式被误判为\omega_1类的错误率
  $$

- **耐曼-皮尔逊决策出发点：**在$P_2(e)$等于常数的条件下，使$P_1(e)$为最小，以此确定阈值t

### 判别式

- 辅助函数：$Q=P_1(e)+\mu P_2(e)$，式中，$\mu$为待定常数，$P_2(e)$为常数

- **决策规则**：
  $$
  若\frac{p(X|\omega_1)}{p(X|\omega_2)}
  \begin{matrix}
  > \\
  <
  \end{matrix}
  \mu ,则X\in
  \left\{
  \begin{aligned}
  \omega_1 \\
  \omega_2 
  \end{aligned}
  \right.
  $$

- 当$\frac{p(X|\omega_1)}{p(X|\omega_2)}=\mu$时，此时的X为**模式的判决阈值t**，是**似然比阈值$\mu$**的函数$X=t(\mu)$，此即两类模式的**判别界面**

- 求解$\mu$ 值：
  $$
  P_2(e)=\int^{t(\mu)}_{-\infty}p(X|\omega_2)dX
  $$




### 计算步骤

1. 求类概率密度
   $$
   如正态分布的类概率密度函数：\\
   p(X|\omega_i)=\frac{1}{(2\pi)^{n/2}|C_i|^{1/2}}exp\{-\frac{1}{2}(X-M_i)^TC^{-1}_i(X-M_i)\}
   $$

2. 求似然比
   $$
   \frac{p(X|\omega_1)}{p(X|\omega_2)}
   $$

3. 求判别式

   决策规则：
   $$
   若 似然比
   \begin{matrix}
   > \\
   <
   \end{matrix}
   \mu ，则X\in 
   \left\{
   \begin{aligned}
   \omega_1 \\
   \omega_2
   \end{aligned}
   \right.
   $$

4. 求似然比阈值$\mu$
   $$
   P_2(e)=\int_{R_1}p(X|\omega_2)dX
   $$




### 研究算法的三种思路

1. 使总错误率最小：最小错误率贝叶斯决策
2. 使风险（错误引起的平均损失）最小：
   - 最小后验风险贝叶斯决策
   - （0-1）损失最小风险贝叶斯决策
3. 限制一类错误概率，追求另一类错误率最小：耐曼-皮尔逊决策



## 4.5 概率密度函数的参数估计方法

- 当类模式的概率密度函数$p(X|\omega_i)$是未知的时候，需要从已知样本中学习，进行估计
- 概率密度的两类估计方法：
  - 参数估计方法：**已知概率密度函数的形式，而函数的有关参数未知**，通过估计参数来估计概率密度函数
    - 主要的参数估计方法：**最大似然估计、贝叶斯估计**
  - 非参数估计方法：**概率密度函数的形式未知**，直接估计概率密度函数的方法

### 4.5.1 最大似然估计

#### 似然函数

​	从$\omega_i$类中独立地抽取N个样本：$X^N=\{X_1,X_2,...,X_N\}$，称这N个样本的联合概率密度函数$p(X^N|\theta)$为相对于样本集$X^N$的$\theta$的似然函数。
$$
p(X^N|\theta)=p(X_1,X_2,...,X_N|\theta)=\prod^N_{k=1}p(X_k|\theta) \\
——在参数\theta 下观测到的样本集X^N的概率（联合分布）密度
$$

#### 最大似然估计

即使得似然函数极大化的$\theta$值，由$\frac{dp(X^N|\theta)}{d\theta}=0$求得。一般用似然函数的对数计算。

设$\omega_i$类模式的概率密度函数有p个未知参数，记为p维向量$\theta=[\theta_1,\theta_2,...,\theta_p]^T$，此时
$$
lnp(X^N|\theta)=\sum^N_{k=1}lnp(X_k|\theta) \\
\frac{\part}{\part\theta}[\sum^N_{k=1}lnp(X_k|\theta)]=0，即\\
\left\{
\begin{aligned}
&\sum^N_{k=1}\frac{\part}{\part\theta_1}p(X_k|\theta)=0 \\
&\sum^N_{k=1}\frac{\part}{\part\theta_2}p(X_k|\theta)=0 \\
&... \\
&\sum^N_{k=1}\frac{\part}{\part\theta_p}p(X_k|\theta)=0
\end{aligned}
\right.
$$
解以上微分方程即可得到$\theta$的最大似然估计值。



### 4.5.2 贝叶斯估计与贝叶斯学习

- 贝叶斯估计和学习都是将位置参数看作随机参数进行考虑



#### 贝叶斯估计

​	通过似然函数，并利用贝叶斯公式，将随机变量$\theta$的先验概率密度$p(\theta)$转变为后验概率密度$p(\theta|X^N)$，然后根据$\theta$的后验概率密度求出估计量

- 步骤

  1. 确定$\theta$的先验概率密度$p(\theta)$

  2. 由样本集$X^N=\{X_1,X_2,...,X_N\}$求出$p(X^N|\theta)$

  3. 利用贝叶斯公式求出后验概率密度：
     $$
     p(\theta|X^N)=\frac{p(X^N|\theta)p(\theta)}{\int p(X^N|\theta)p(\theta)d\theta}
     $$

  4. 求贝叶斯估计量
     $$
     \widehat{\theta}=\int \theta p(\theta|X^N)d\theta
     $$






#### 贝叶斯学习

​	利用$\theta$的先验概率密度及样本提供的信息求出$\theta$的后验概率密度，根据后验概率密度直接求出类概率密度函数$p(X|\omega_i)$

- 迭代式：
  $$
  p(\theta|X^N)=\frac{p(X_N|\theta)p(\theta|X^{N-1})}{\int p(X_N|\theta)p(\theta|X^{N-1})d\theta}
  $$




- 步骤：

  1. 根据先验知识得到$p(\theta)$的初始估计：$p(\theta)=p(\theta|X^0)$

  2. 用$X_1$对初始的$p(\theta)$进行修改：
     $$
     p(\theta|X^1)=p(\theta|X_1)=\frac{p(X_1|\theta)p(\theta)}{\int p(X_1|\theta)p(\theta)d\theta}
     $$

  3. 给出$X_2$，对用$X_1$估计的结果进行修改

  4. 逐次用样本进行迭代修改

## 4.6 概率密度函数的非参数估计方法

### 基本方法

- 出发点：

  若观察到：N个样本中有k个落入区域R中，则可以合理认为：

$$
\widehat{P}\approx k/N
$$

​	设$p(X)$连续，区域R足够小，且体积为V，$p(X)$在R中没有变化，X是R中的点，则X点概率密度的估计：
$$
\widehat{p}(X)\approx \frac{k/N}{V}
$$

- 存在的问题：

  - 固定V ，样本数增多，则k/N以概率1收敛。但只能得到在某一体积V中的平均估计。
  - N固定，V趋于零，p(X) = 0 或发散到无穷大。没有意义。

- 估计的步骤：

  - 构造一串包含X的区域$R_1,R_2,...,R_N,...$

  - 对$R_1$采用一个样本估计，对$R_2$采用两个样本，...

  - 假定$V_N$是$R_N$的体积，$k_N$是落入$R_N$内的样本数目，$\widehat{p}_N(X)$是$p(X)$的第N次估计，有
    $$
    \widehat{p}_N(X)=\frac{k_N/N}{V_N}
    $$

- 为保证估计合理性，应满足三个条件：
  $$
  1.\;\;lim_{N\rightarrow\infty}V_N=0 \\
  2.\;\; lim_{N\rightarrow\infty}k_N=\infty \\
  3. \;\; lim_{N\rightarrow\infty}k_N/N=0
  $$

- 两种非参数估计法：**Parzen窗法，$k_N$近邻估计法**



### 4.6.2 Parzen窗法

#### 基本概念

$$
设区域R_N:d维超立方体，棱长：h_N,则 \\
V_N=h^d_N \\
定义窗函数\varphi(\mu)=
\left\{
\begin{aligned}
&1,\quad 当|\mu_j|\le \frac{1}{2};j=1,2,..,d \\
&0,\quad 其他
\end{aligned}
\right. \\
当X落入以X_i为中心，体积为V_N的超立方体时：\\
\phi_i(X)=\phi[(X-X_i)/h_N]=1 \\
否则，\phi_i(X)=0 \\
落入超立方体内的样本数为\\
k_N=\sum^N_{i=1}\varphi(\frac{X-X_i}{h_N}) \\
代入\widehat{p}_N(X)=\frac{k_N/N}{V_N}得 \\
\widehat{p}_N(X)=\frac{1}{N}\sum^N_{i=1}\frac{1}{V_N}\varphi(\frac{X-X_i}{h_N})\quad ——Parzen窗法基本公式
$$

- **实质**：

​	窗函数的作用是内插（平滑），样本对估计所起的作用取决于它到X的距离

- $\varphi(\mu)$应满足的两个条件：
  1. $\varphi(u)\ge 0$
  2. $\int\varphi(u)du=1$

#### 窗函数的选择

![1](images/1.PNG)

- 窗宽$h_N$对估计量$\widehat{p}_N(X)$的影响很大
- 



# 5. 特征提取与选择

## 5.2 类别可分性测度

- 类别可分性测度：衡量类别间可分性的尺度
  - 几何分布：类内距离和类间距离
  - 概率分布：类概率密度函数
  - 误判概率：与错误率有关的距离

### 5.2.1 基于距离的可分性测度

#### 类内散布距离和类内散布矩阵

1. $\omega_i$类的类内距离一：类内（样本间）均方两两距离
   $$
   \overline{D_i^2}=E[||X_k-X_l||^2|\omega_i]=E[(X_k-X_l)^T(X_k-X_l)|\omega_i] \\
   其中X_k和X_l：均为同一类模式样本
   $$
   若{$X_i$}中样本相互独立，则：
   $$
   \overline{D_i^2}=2Tr[C_i]=2\sum^n_{k=1}\sigma^2_k
   $$

2. $\omega_i$类的类内距离二：类内（样本间）均方散布距离
   $$
   \overline{D_{w,i}^2}=E[||X_k-M_i||^2\ |\ X_k\in \omega_i] \\
   M_i:第i类的均值向量
   $$

3. 两个类内距离的计算式
   $$
   \overline{D_{w,\ i}^2}=E[||X_k-M_i||^2\ |\ \omega_i]=\frac{1}{n}\sum^{n_i}_{k=1}(X_k^i-M_i)^T(X_k^i-M_i) \\
   \overline{D^2_w}=\sum^c_{i=1}P(\omega_i)D_{w,i}^2=\sum^c_{i=1}P(\omega_i)[\frac{1}{n}\sum^{n_i}_{k=1}(X^i_k-M_i)^T(X^i_k-M_i)] \\
   \overline{D^2_i}=E[{(X_k-X_l)^T(X_k-X_l)|\omega_i}]=\frac{1}{n_i(n_i-1)}\sum^{n_i}_{k=1}\sum^{n_i}_{l=1,l\ne k}(X^i_k-X^i_l)^T(X_k^i-X_l^i)
   $$




1. 类内散布矩阵：表示各样本点围绕类均值的散布情况——即第i类分布的**协方差矩阵**：
   $$
   S_{w,i}=E[(X-M_i)(X-M_i)^T|\omega_i]=\frac{1}{n}\sum^{n_i}_{k=1}(X_k^i-M_i)(X_k^i-M_i)^T \\
   可知，\overline{D^2_{w,i}}=Tr(S_{w,i}) \\
   计算： \\
   C=\frac{1}{n}\sum^n_{i=1}X_iX^T_i-MM^T
   $$
   **特征选择和提取的结果应使类内散布矩阵的迹越小越好**

#### 类间散布距离和类间散布矩阵

1. 类间距离一：各模式类均值向量之间**距离平方的加权和**
   $$
   \overline{D^2}=\frac{1}{2}\sum^c_{i=1}P(\omega_i)\sum^c_{j=1}P(\omega_j)||M_i-M_j||^2
   $$




1. 类间距离二：各模式类均值向量与总体均值向量之间距离平方的加权和——类间均方散布距离
   $$
   \overline{D_b^2}=\sum^c_{i=1}P(\omega_i)(M_i-M_0)^T(M_i-M_0) \\
   其中，M_0:所有c类模式的总体均值向量
   $$

2. 类间散布矩阵：表示c类模式在空间的散布情况：
   $$
   S_b=\sum^c_{i=1}P(\omega_i)(M_i-M_0)(M_i-M_0)^T
   $$
   类间散布距离和类间散步矩阵的关系：
   $$
   \overline{D_b^2}=Tr(S_b)
   $$
   **类间散布举证的迹越大越有利于分类。**



#### 多类模式向量间的总体距离和总体散布矩阵

##### 两类情况的距离

$$
设\omega_1类有q个样本，\omega_2类中有p个样本，则 \\
两个类之间的距离=p\times q个点间距离的平均值
$$

##### 多类情况的总体距离

1. 多类模式向量间的总体均方距离：
   $$
   J_d=\frac{1}{2}\sum^c_{i=1}P(\omega_i)\sum^c_{j=1}P(\omega_j)\frac{1}{n_in_j}\sum^{n_i}_{k=1}\sum^{n_j}_{l=1}D^2(X_k^i,X_l^j) \\
   其中，n_i和n_j：\omega_i和\omega_j类的样本数； \\
   D^2(X^i_k,X_l^j):X_k^i和X_l^j间欧氏距离的平方
   $$

2. 总体均方距离的另一种形式：
   $$
   J_d=\sum^c_{i=1}P(\omega_i)[\frac{1}{n}\sum^{n_i}_{k=1}(X_k^i-M_i)^T(X_k^i-M_i)]+\sum^c_{i=1}P(\omega_i)[(M_i-M_0)^T(M_i-M_0)] \\
   =\overline{D_w^2}+\overline{D_b^2} \\
   即： 多类模式向量之间的总体均方距离=
   \left\{
   \begin{aligned}
   模式类内散&布距离加权和 \\
   & + \\
   模式类间散&布距离加权和
   \end{aligned}
   \right.
   $$




1. 多类情况的散布矩阵：

   - 多类类间散布矩阵：

   $$
   S_b=\sum^c_{i=1}P(\omega_i)(M_i-M_0)(M_i-M_0)^T
   $$

   - $\omega_i$类内散布矩阵：
     $$
     S_{w,i}=E[(X-M_i)(X-M_i)^T|\omega_i]=\frac{1}{n}\sum^{n_i}_{k=1}(X_k^i-M_i)^T
     $$

   - 多类类内散布矩阵：
     $$
     S_w=\sum^c_{i=1}P(\omega_i)\frac{1}{n}\sum^{n_i}_{k=1}(X_k^i-M_i)(X_k^i-M_i)^T
     $$

   - 多类总体散布矩阵：
     $$
     S_t=E[(X-M_0)(X-M_0)^T]=S_b+S_w
     $$

2. 多类模式的总体均方距离$J_d$与总体散布矩阵$S_t$的关系：
   $$
   J_d=tr(S_t)=tr(S_b+S_w)
   $$






### 6.2.2 基于概率分布的可分性测度

#### 散度

##### 散度定义

散度=两类间对数似然比期望值之和
$$
\omega_i类对\omega_j类的散度定义为J_{ij}: \\
J_{ij}=I_{ij}+I_{ji}=\int_X[p(X|\omega_i)-p(X|\omega_j)]ln\frac{p(X|\omega_i)}{p(X|\omega_j)}dX
$$
散度表示了区分$\omega_i$类和$\omega_j$类的总的平均信息。

##### 散度性质

1. $J_{ij}=J_{ji}$

2. $J_{ij}$为非负，即$J_{ij}\ge 0$

   - $J_{ij}$越大，$p(X|\omega_i)$与$p(X|\omega_j)$相差越大

3. 错误率分析中，两类概率密度曲线交叠越少，错误率越小

4. 散度具有独立可加性，即当各分量相互独立时，有：
   $$
   J_{ij}(X)=J_{ij}(x_1,x_2,...,x_n)=\sum^n_{k=1}J_{ij}(x_k)
   $$

5. 加入独立的新特性不会使散度减小

##### 两个等方差正态分布模式类的散度

$$
\omega_i类和\omega_j类的概率密度函数为：\\
p(X|\omega_i)\sim N(M_i,C) \\
p(X|\omega_i)\sim N(M_i,C) \\
则，\omega_i类对\omega_j类的散度为 \\
J_{ij}=(M_i-M_j)^TC^{-1}(M_i-M_j)
$$



## 5.3 基于类内散布矩阵的单类模式特征提取

- 特征提取的目的：

  - 对某类模式：压缩模式向量的维数
  - 对多类分类：压缩模式向量的维数；保留类别间的鉴别信息，突出可分性

- 特征提取方法：
  $$
  若\{X\in \omega_i\}是\omega_i类的一个n维样本集，将X压缩成m维的向量X^*\\——寻找一个m\times n矩阵A，做变换：\\
  X^*=AX
  $$




### 确定变换矩阵

- 根据类内散布矩阵确定变换矩阵A

$$
设\omega_i类模式的均值向量为M，类内散布矩阵（协方差矩阵）为C：\\
M=E\{X\} \\
C=E\{(X-M)(X-M)^T\} \\
设矩阵C的n个特征值分别为\lambda_1,\lambda_2,...,\lambda_n，其分别对应的特征向量为u_k,则u_k满足 \\
Cu_k=\lambda_ku_k \ \ 的一个非零解 \\
选择n个归一化特征向量作为A的行，则A为归一化正交矩阵：\\
A=
\begin{bmatrix}
u_1^T \\
u_2^T \\
... \\
u_n^T
\end{bmatrix}
\quad\quad 
A^T=[u_1\ u_2 \ ...\ u_n]
\quad\quad
AA^T=I
$$

​	**$X^*$的第k个分量的方差等于未变换时C的特征值$\lambda_k$**

