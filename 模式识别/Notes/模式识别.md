[TOC]

# 模式识别

> 教材：齐敏等，模式识别导论，清华大学出版社， 2009年 
>
> 简明扼要，条理清晰。适合教学型阅读。本课程教材。 

笔记来源：课本+ppt+软软笔记。

2-5章转载改编自软软。

# 1. 绪论

## 1.1 基本概念

- 样本(sample)：一个具体的研究对象，具有一个或多个可观测量。 

- 特征(features)：能从某个方面对样本进行描述、刻画或表达的可观 

  测量(数值型、结构型)。多个特征通常用矢量表示——特征矢量。 

- 模式(pattern)：样本特征矢量的观测值，是抽象样本的数值代表。 在这个意义上，“样本”与“模式”说的是同一件事情。

- 类别、类属、模式类(class)：指在一定合理颗粒度下、有实际区分 意义的基础上，主观或客观地被归属于“同一类” 的客观对象(样本 、模式)的类别代号。数学上一般处理为整数。 

- 样本集(sample set)：多个样本的集合。训练集、验证集、测试集。 

- 已知样本(known samples，example)：事先知道类别标号的样本。 

- 未知样本(unknown samples)：指特征已知、但类别标号未知的样 本，也称:待分样本、待识样本。 

- 在机器学习中，称已知样本为**样例**，未知样本为**实例**。 

- 模式识别(pattern recognition)：基于样本和应用目的设计分类器; 

  并对性能进行验证，进行必要优化；最后用分类器对未知类属样本进 行类属判定。识别，不是宽泛的认识或理解，而是对类别判定。 

## 1.2 模式识别系统的组成

![](images/1-2.png)

**模式识别系统的主要环节：**

- 特征抽取：特征数据的采集。如长度测量、波形测量、...
- 特征选择：选择尽可能少、且更有利于分类的特征。含特征变换(提取)
- 分类学习：利用样本集建立分类和识别规则，即分类器设计
- 识别分类：对所获得样本按建立的分类规则进行分类识别

### 概念

- 训练集：样本的集合，用它来设计开发模式分类器。在有监督分 类中，已知类别样本的集合，在无监督分类中，是未知类别样本 的集合。但都被用来提取“关于各类之间的分类知识”，即使这 些样本本身没有类别信息，仍然隐含着某种分类信息! 

- 测试集：在设计分类系统时没有用过的独立样本集 
- 系统评价原则：为了更好地对模式识别系统性能进行评价，必须使用一组独立于训练集的测试集对系统进行测试。

#### “处理”与“识别”两个概念的区别

![](images/1-2-2.png)

- 处理：输入与输出是同样性质的数值或特征。
- 识别：输入的是对象特征，输出的是它的分类和描述。

## 1.3 模式识别方法的分类

### 模式识别：按理论基础划分

-  **统计模式识别：以模式特征的概率分布为基础进行统计分析**
- 结构模式识别：以形式语言理论对模式的结构特征进行分析
- 模糊模式识别：以模糊数学理论与方法进行的模式分类
- 神经网络模式识别：以人工神经网络理论与方法为基础
- 人工智能模式识别：以逻辑推理与专家系统理论为基础 

### 统计模式识别：按任务类型划分

- 聚类分析(Clustering Analysis)——简称：聚类 
  - 简单聚类方法：最大最小距离法
  - 层次聚类方法：分裂式、凝聚式
  - 动态聚类方法：K-均值，ISODATA 

- 判别分析(Discriminatory Analysis)——简称:分类
  - 几何分类法(判别函数分类法)：线性、分段线性、二次、支持向量机 
  - 概率分类法(统计决策分类法)：判别式Discriminative、生成式Generative 
  - 近邻分类法(几何分类法和概率分类法的一种融合方法) 

### 模式识别：按实现方式划分

- 有监督分类：有已知类别的样本集用于分类器设计。需要有足够的先验知识。一般就指判别分析。 

- 无监督分类：用不知类别的样本集进行分类器设计。需要某种意义的先验知识和人工判断。主要指聚类分析，还包括降维、流形学习 、MDS等探索性数据分析方法。 

# 2. 聚类分析

## 2.2 相似性测度和聚类准则

### 2.2.1 相似性测度

- 相似性测度：衡量模式之间相似性的一种量度。 
- 由**n个特征值组成的n维向量 $X = [x_1 , x_2 ,....,x_n ]^T$ ，称为该模式(样本)的==特征矢量==**。它相当于特征空间中的一个点，以特征空间中的点间距离作为模式相似性的测量，作为模式归类依据，距离越小，越“相似”。

#### 1. 欧氏距离

> 设$X_1$、$X_2$为两个n维模式样本，$X_1=[x_{11},x_{12},...,x_{1n}]^T, X_2=[x_{21},x_{22},...,x_{2n}]^T$ 
>
> 欧氏距离定义为：
>
> $D(X_1,X_2) = ||X_1-X_2||=\sqrt{(X_1-X_2)^T(X_1-X_2)}=\sqrt{(x_{11}-x_{21})^2+...+(x_{1n}-x_{2n})^2}$
>
> 距离越小，越相似。

注意：

- 各特征维上应当是相同的物理量（$x_{11}$和$x_{21}$应该是一样的物理量）。
- 注意同类物理量的量纲应该一样。

#### 2. 马氏距离

> $D^2=(X-M)^TC^{-1}(X-M)$
>
> X：模式向量；M：均值向量；
>
> C：该类模式总体的协方差矩阵

- 在各分量特征维度计算**样本模式与均值模式的距离**上，**剔除了该维度上模式类的方差影响（乘以协方差矩阵的逆）**——方差大，说明模式取值变化大，因此计算距离时要用方差归一化，这样得出的分量模式与均值模式的距离才具有比较意义。
- 优点:排除了模式样本之间的相关性影响 。
- 特例:当C = I 时，马氏距离退化为欧氏距离。 

**协方差矩阵**：

对n维向量：$X = 
\left[
\begin{matrix}
x_1 \\
... \\
x_n
\end{matrix}
\right] 
$ , $M = 
\left[
\begin{matrix}
m_1 \\
... \\
m_n
\end{matrix}
\right] 
$
$$
\begin{align*}
协方差矩阵 C &= E\{(X-M)(X-M)^T\} \\
&= E\{ 
\left[
\begin{matrix}
(x_1-m_1) \\
(x_2-m_2) \\
... \\
(x_n-m_n)
\end{matrix}
\right] 

\left[
\begin{matrix}
(x_1-m_1) (x_2-m_2) ... (x_n-m_n)
\end{matrix}
\right] 

\}

\\
&= 
\left[
\begin{matrix}
E(x_1-m_1)(x_1-m_1) \,\, E(x_1-m_1)(x_2-m_2) \,\,...\,\, E(x_1-m_1)(x_n-m_n) \\
E(x_2-m_2)(x_1-m_1) \,\, E(x_2-m_2)(x_2-m_2) \,\,...\,\, E(x_2-m_2)(x_n-m_n) \\
.  .  . \\
E(x_n-m_n)(x_1-m_1) \,\, E(x_n-m_n)(x_2-m_2) \,\,...\,\, E(x_n-m_n)(x_n-m_n) \\
\end{matrix}
\right] 

\\
&= 
\left[
\begin{matrix}
\sigma^2_{11} \,\, \sigma^2_{12} \,\,...\,\, \sigma^2_{1n} \\
\sigma^2_{21} \,\, \sigma^2_{22} \,\,...\,\, \sigma^2_{2n} \\
.  .  . \\
\sigma^2_{n1} \,\, \sigma^2_{n2} \,\,...\,\, \sigma^2_{nn} \\
\end{matrix}
\right] 
\end{align*}
$$

#### 3. 明氏距离（m-范数）

> $D_m(X_i,X_j)=[\sum^n_{k=1}|x_{ik}-x_{jk}|^m]^{1/m}$
>
> 也称为**m-范数**。

- 当m=2时，明氏距离为欧氏距离。

- 当m=1时，称为“街坊”距离：

  $D_1(X_i,X_j) = \sum^n_{k=1}|x_{ik}-x_{jk}|$

![1](images/2-2-1.png)

#### 4. 汉明距离

> 设$X_i$和$X_j$为n维二值模式（分量取值1或-1）样本向量，则
>
> $D_h(X_i,X_j)=\frac {1}{2}(n-\sum^n_{k=1}x_{ik}·x_{jk})$ 

- $D_h(X_i,X_j) = X_{ik}和X_{jk}取不同值的个数 = 不相似性$ 
- 该式中的**求和式**，表达的是两个二值向量之间，同值分量（即值相等的分量）与不同值分量数之差，**该差值越大，则两个向量越不相似**。
- 当这个差值取值为：
  - 0：表示两模式向量的各个分量相同
  - n：表示两模式向量的各个分量都不同
  - n/2：表示两模式分量中，取值相同的分量数比取值不同的分量数多n/2

#### 5. 角度相似性函数

> $S(X_i,X_j)=\frac{X^T_iX_j}{||X_i||·||X_j||}$
>
> 即两向量之间夹角的余弦。



#### 6. Tanimoto测度

广义Jaccard系数。

> 用于0-1型二值特征情况，定义：
>
> $S(X_i,X_j)=\frac{X_i^TX_j}{X^T_iX_i+X^T_jX_j-X^T_iX_j}=\frac{X_i,X_j之间具有某特征的数量}{X_i,X_j中有某种特征的分量总数}$

- 如果把记1为“有某种特征”，记0为“无某种特征”，则Tanimoto测度的分子表达的是两特征向量之间“都具有某种特征”的分量数，分母表达的则是，在两个向量中“至少有一个向量的分量有某种特征”的分量数。

### 2.2.2 聚类准则

- 确定聚类准则的两种方式：

  - 阈值准则：根据规定的距离阈值进行判断
  - 函数准则：利用聚类准则函数进行判断

- 聚类准则函数：

  $J=\sum^c_{j=1}\sum_{X\in S_j}||X-M_j||^2$         ——误差(距离)平方和

  - c为聚类类别的数目
  - $M_j=\frac{1}{N_j}\sum_{x\in S_j}X$是属于$S_j$集的样本的均值向量，$N_j$为模式类$S_j$中样本数目
  - J 代表了分属于c个聚类类别的**全部模式样本**与其相应类别模式**均值样本**之间的**误差(距离)平方和**。
  - 适用于各类样本密集且数目相差不多，而不同类间的样本又明显分开的情况。

## 2.3 基于距离阈值的聚类算法

### 2.3.1 近邻聚类法

**重点：聚类中心的判定。**

- 算法描述
  1. 任取样本$X_i$作为第一个聚类中心的初始值$Z_1$。
  2. 计算样本$X_2$到$Z_1$的欧氏距离D，若D>T，定义一新的聚类分析$Z_2=X_2$，否则$X_2$属于以$Z_1$为中心的聚类
  3. 设已有聚类中心$Z_1, Z_2$, 计算$D_{31}$和$D_{32}$，若$D_{31}>T$且$D_{32}>T$，则建立第三个聚类中心，否则$X_3$属于离$Z_1,Z_2$中最近者。
  4. 依次类推。

### 2.3.2 最大最小距离算法

又称小中取大距离算法。

**本质：找出C个聚类中心。**

- 思路总结：
  - 二步法。先找全部中心，然后再对剩余模式归类。关键:怎样开新类，
     聚类中心如何定。
  - 为使聚类中心更有代表性，可取各类的**样本均值**作为聚类中心。

- 算法描述
  1. 选任意一模式样本作为第一聚类中心$Z_1$。
  2. 选择**离$Z_1$距离最远的样本**作为第二聚类中心$Z_2$。
  3. 逐个计算各模式样本$X_i$与已确定的所有聚类中心$Z_i$之间的距离，并选出其中最小的距离。
  4. 在**所有最小距离中选出最大距离**，如该最大值达到$||Z_1-Z_2||$的一定分数比值以上，则相应的样本点取为新的类中心。
  5. 重复，直到没有新的聚类中心出现为止。
  6. 将剩余样本按最近距离划分到相应的聚类中心对应的类中。

### 2.3.3 层次聚类法

又称系统聚类法、谱系聚类法。

- 算法描述

  1. N个初始模式样本自成一类，即建立N类：

     $G_1(0),G_2(0),...,G_N(0)$

     计算各类之间的距离，得一N*N维距离矩阵D(0)，0表示初始状态

  2. 假设已求的距离矩阵D(n)，找出D(n)中的最小元素，将其对应的两类合并为一类，由此建立新的分类：

     $G_1(n+1),G_2(n+1),...$

  3. 再次计算：经过合并后，各个类别之间的距离，得D(n+1)

  4. 跳至第2步，重复计算及合并。

  **结束条件：**

  1. 取距离阈值T，当D(n)的最小分量超过给定值T时，算法停止，所得即为聚类结果；
  2. 若不取阈值，则直到全部样本聚为一类为止，输出聚类的分级树（得到全部可能的聚类结果——谱系聚类的名字由来。）

- 类间距离计算方法

  - 最短距离法：算出分别属于两个聚类中两个样本的欧氏距离，**比较所有的欧氏距离，以最小的距离作为类间距离**。

  - 最长距离法：类比上。

  - 中间距离法：若K类由I类和J类合并而成，则H和K类之间的距离为：

    $D_{HK}=\sqrt{\frac{1}{2}D^2_{HI}+\frac{1}{2}D^2_{HJ}-\frac{1}{4}D^2_{IJ}}$

  - 重心法

    $D_{HK}=\sqrt{\frac{n_I}{n_I+n_J}D^2_{HI}+\frac{n_J}{n_I+n_J}D^2_{HJ}-\frac{n_In_J}{(n_I+n_J)^2}D^2_{IJ}}$

  - 类平均距离法

    $D_{HK}=\sqrt{\frac{1}{n_Hn_K}\sum_{i\in H,j\in K}d^2_{ij}}$

    $d^2_{ij}$：H类任一样本$X_i$和K类任一样本$X_j$之间的欧氏距离平方

## 2.5 动态聚类法

**动态含义：聚类过程中，聚类中心位置或个数不断发生变化。**

两种常用算法: 

1. K-均值算法(或C-均值算法)**（重点）**
2. ISODATA算法(迭代自组织数据分析算法) （略）
    (Iterative Self-Organizing Data Analysis Techniques Algorithm) 

![2-5](images/2-5.png)

### 2.5.1 K-均值算法

- 思路描述

  **K-均值算法的聚类准则:聚类中心的选择，应使准则函数$J$极小，也即：使$J_j$的值极小。**

  由推导可知，$S_j$类的**聚类中心**应选为该类样本的**均值向量**。

  > 基于使聚类准则函数最小化，其准则函数: 聚类集中每一样本点到该类中心的距离平方和。 
  >
  > 对于第j个聚类集，准则函数定义为： 
  >
  > ​			$J_j = \sum ^{N_j}_{i=1} ||X_i-Zj|| ^2, X_i\in S_j $
  >
  > ​			$S_j$: 第j个聚类集(域)，聚类中心为$Z_j$ ; 
  >
  > ​			$N_j$: 第j个聚类集$S_j$中所包含的样本个数。
  >
  >  对所有K个模式类有： 
  >
  > ​			$J=\sum^k_{j=1}  \sum ^{N_j}_{i=1} ||X_i-Zj|| ^2, X_i\in S_j $ 
  >
  > 应有 	$\frac {\part J_j}{\part Z_j} = 0 $
  >
  > 即  $\frac{\part}{\part Z_j} \sum^{N_j}_{i=1} ||X_i - Z_j||^2 = \frac{\part}{\part Z_j} \sum^{N_j}_{i=1} (X_i - Z_j)^T(X_i - Z_j) = 0 $
  >
  > 得 $Z_j = \frac{1}{N_j} \sum^{N_j}_{i=1} X_i, X_i \in S_j$
  >
  > 上式表明，$S_j$类的聚类中心应选为该类样本的**均值向量**。

- 算法描述

  1. 任选K个初始聚类中心：$Z_1(1),Z_2(1),...,Z_K(1)$（括号内为迭代的次序号）

  2. 按最小距离原则将其余样本分配到K个聚类中心中的一个。

     (注意：k——迭代运算次序号；**K——聚类中心的个数**。)

  3. 再次计算各个聚类中心的新向量值$Z_j(k+1)$，新聚类中心为均值向量：$Z_j(k+1) = \frac{1}{N_j} \sum_{X_i \in S_j(k)} X, \,\,  j = 1,2,...,K $。

  4. 如果$Z_j(k+1) \ne Z_j(k)$，则回到2.，将模式样本逐个重新分类，重复迭代计算。

     如果$Z_j(k+1) = Z_j(k)$，算法收敛，计算完成。

- 根据$J_j-K$关系曲线，曲线拐点对应着接近最优的K值。

## 2.6 聚类结果的评价

1. 评价的重要性 
   - 对高维特征向量样本，不能直观看清聚类效果。
   - 人机交互系统中，需要迅速地判断中间结果，及时指导输入参数的改变，较快地获得较好的聚类结果。 

2. 常用的几个指标
   - 聚类中心之间的距离——越大越好。 
   - 各个聚类域中样本数目——尽可能相差不大。
   - 各个聚类域内样本的标准差向量——尽可能小。 

- 以上都是原则性要求。实际情况下，受模式实际分布情况的影响，需要综合权衡。比如：实际情况中的样本集的确包含两个类，但是类间距离本身不大，且类内方差偏大，如果简单按聚类准则函数的极值进行优化求解，很可能把本来属于同一类的模式强行分为两类!

# 3. 判别函数分类法

## 3.1 判别函数

- 统计模式识别：按**任务类型**划分：
  - 聚类分析
  - 判别分析
    - 几何分类法
    - 概率分类法
    - 近邻分类法
- 判别函数定义：直接用来对模式进行分类的决策函数。
- 判别函数正负值的确定：是在训练判别函数的权值时**人为确定的**。

## 3.2 线性判别函数

- 一般形式：

  $d(X)=w_1x_1+w_2x_2+...+w_nx_n+w_{n+1}=W^T_0X+w_{n+1}$

  式中：$X=[x_1,x_2,...,x_n]^T$，$W_0=[w_1,w_2,...,w_n]^T$：权向量，即参数向量。

- 增广向量形式：

  $d(X)=w_1x_1+w_2x_2+...+w_nx_n+w_{n+1}·1=[w_1 w_2 ... w_n w_{n+1}]\left[\begin{matrix} x_1 \\ x_2 \\...\\x_n\\1 \end{matrix}\right]=W^TX$

### 3.2.2 线性判别函数的性质

#### 1. 两类情况

$$
d(X)=W^TX \left\{
\begin{aligned}
>0,若X\in w_1 \\
<0,若X\in w_2
\end{aligned}
\right.
$$

#### 2. 多类情况

- 对M个线性可分模式类，$w_1,w_2,...,w_M$，有三种分类方式：

  - $w_i/\overline{w_i}$，是非两分法 ==> 一对多
  - $w_i/w_j$，成对两分法 ==> 一对一
  - $w_i/w_j$，成对两分法特例（没有不确定区） ==> 一对一

##### 2.1 多类情况1-------是非两分法
$$
  d_i(X)=W^T_iX\left\{
  \begin{aligned}
  >0, 若X\in w_i \\
  <0, 若X\in \overline{w_i}
  \end{aligned}
  \right.   i=1,..., M
$$

- 用线性判别函数将属于$w_i$ 类的模式与其余不属于 $w_i$ 类的模式分开。能用本方法分类的模式集称为：**整体线性可分的**。 
- 将某个待分类模式X 分别代入M 个类的d (X)中，
    **若只有$d_i(X)>0$，其他$d_j(X)$均<0，则判为$w_i$类**。
- 此法将M类问题分成M个两类问题，识别每一类均需M个判别函数。**识别出所有的M类仍是这M个函数。**
- ![3](images/3.PNG)

##### 2.2 多类情况2-------成对两分法

- 一个判别界面只能分开两个类别，不需要把其余所有的类别都分开。

- 判决函数：$d_{ij}(X)=W^T_{ij}X$，这里$d_{ji}=-d_{ij}$

  $d_{ij}(X)>0, \forall j\ne i; i,j =1,2,...,M, 若X\in w_i$

- 在M类模式中，**以i开始的**M-1个判决函数全为正时，$X\in w_i$，其中若有一个为负，则为IR区

- 能用本方法分类的模式集称为：**成对线性可分的**。

- 每分离出一类，需要与模式有关的M-1个判别函数，**M类共需要$C^2_M=\frac{M(M-1)}{2!}$ 个判决函数**，因此计算复杂。

##### 2.3 多类情况3-------成对两分法特例（没有不确定区）

- 若成对两分法中的判别函数，可以分解为
  $$
  d_{ij}(X)=d_i(X)-d_j(X)
  $$
  时，那么$d_i(X)>d_j(X)$就相当于成对两分法中的$d_{ij}(X)>0$

  此时，对M类情况，等价的判别函数性质为：

  $d_i(X)>d_j(X), \forall j\ne i; i,j =1,2,...,M, 若X\in w_i$

  或描述为：$d_i(X) = max\{d_k(X),\,\, k =1,2,...,M\}, 则决策X\in w_i$

- 能用本方法分类的模式集称为：**线性可分的**。

- 若在第三种情况下可分，则在第二种情况下也可分，但反之不一定成立。

- 除边界区外，**没有不确定区域**。

- $w_i/\overline{w_i}（是非两分法）与w_i/w_j（成对两分法）$两分法的比较：

  - $w_i/\overline{w_i}$两分法共需要**M**个判别函数，$w_i/w_j$需要**M(M-1)/2**个
  - 当M>3时，后者需要更多的判别式，但对模式集进行线性可分的可能性要更大一些
  - 一种类别模式$w_j$的分布要比M-1类模式$\overline{w_i}$的分布更为聚集，因
    此$w_i/w_j$两分法受到的限制比$w_i/\overline{w_i}$少，因此线性可分可能性大。

## 3.3 广义线性判别函数

- 广义线性判别的目的：

  - 通过某映射，把模式空间X变成X\*，以便将X空间中非线性可分的模式集变成在X\*空间中的线性可分的模式集。

- 非线性多项式判别函数

  - 设一训练用模式集，{X}在模式空间X中线性不可分，非线性多项式判别函数形式如下：

    $d(X)=w_1f_1(X)+w_2f_2(X)+...+w_kf_k(X)+w_{k+1}=\sum^{k+1}_{i=1}w_if_i(X)$

    式中$\{f_i(X), i = 1,2,...,k\}$ 是模式X的单值实函数，$f_{k+1}(X) = 1$。

    - $f_i(X)$取什么形式以及d(X)取多少项，取决于非线性边界的复杂程度。

- 广义线性函数的模式向量定义为：

  $X^*=[x_1^*,x_2^*,...,x_k^*,1]^T=[f_1(X),f_2(X),...,f_k(X),1]^T$

  这里X\*空间的维数k高于X空间的维数n，$d(X)=W^TX ^*= d(X^*),\,\, W=[w_1,w_2,..,w_k,w_{k+1}]^T$。

- 存在的问题：

  - 非线性变换可能非常复杂。
  - 维数大大增加，导致维数灾难。

## 3.4 线性判别函数的几何性质

### 3.4.1 模式空间与超平面

概念：

- **模式空间**：以n维模式向量X的n个分量为坐标变量的欧氏空间。

- **模式向量**：点、有向线段。

- **线性分类**：用d(X)进行分类，相当于用超平面d(X)=0把模式空
  间分成不同的决策区域。

- 设判别函数：$d(X)=W_0^TX+w_{n+1}$，超平面：$d(X)=W_0^TX+w_{n+1}=0$。

- **超平面的法向量**：

  - X1,X2在超平面上：

  ![4](images/4.PNG)

  ​	$W_0^T(X_1-X_2)=0$，W0即超平面d(X)的法向量。

  ​	**超平面的单位法线向量为U：**

  ​	$U=\frac{W_0}{||W_0||}$，$||W_0||=\sqrt{w_1^2+w_2^2+...+w_n^2}$

  - X不在超平面上：

    ![5](images/5.PNG)

    将X向超平面投影得向量$X_p$，构造向量R（投影点$X_p$到X的向量）：

    > $R=r·U = r \frac{W_0}{||W_0||}$	     r：X到超平面的**代数距离**。（即r带正负号）
    >
    > 有 $X = X_p + R = X_p + r\frac{W_0}{||W_0||}$
    >
    > $d(X) = W_0^T(X_p+ r\frac{W_0}{||W_0||})+w_{n+1} = (W_0^TX_p+w_{n+1})+W_0^T· r\frac{W_0}{||W_0||} = r||W_0||$
    >
    > ($W_0^TW_0 = ||W_0||^2$)

    **判别函数d(X)正比于点X到超平面的代数距离**

    $r=\frac{d(X)}{||W_0||}$

  - X在原点：

    > $d(X)=W_0^TX+w_{n+1} = w_{n+1}$

    $r_0=\frac{w_{n+1}}{||W_0||}$

    **原点在超平面的正负侧位置由阈值权$w_{n+1}$决定**。

### 3.4.2 权空间与权向量解

#### (1) 概念

- **权空间**：以$d(X)=w_1x_1+w_2x_2+...+w_nx_n+w_{n+1}$的权系数为坐标变量的（n+1）维欧氏空间，X为已知。*（X已知而W未知时，由X求W，W为变量）*
- **增广权向量**：点、有向线段。

#### (2) 线性分类

判别函数形式已定，只需确定权向量。

设增广样本向量：
$$
w_1类：X_{11},X_{12},...,X_{1p} \\
w_2类：X_{21},X_{22},...,X_{2q}
$$
使d(X)将w1和w2分开，必须使下面(p+q)个不等式成立：
$$
d(X_{1i})>0,i=1,2,...,p \\
d(X_{2i})<0,i=1,2,...,q
$$
给w2的q个增广模式乘-1，统一为：
$$
d(X)>0,其中X=\left\{
\begin{aligned}
X_{1i},i=1,2,...,p \\
-X_{2i},i=1,2,...,q
\end{aligned}
\right.
$$
这就是**样本的符号规范化**，X：**规范化增广样本向量**。

对每个已知的X，d(X)=0在权空间确定一个超平面；**(p+q)个X共确定(p+q)个超平面**。

#### (3) 解空间

- 在权空间中寻找权向量W使判别函数d(X)能把ω1类和ω2类分开，这样的权向量，必定**在(p+q)个超平面的正侧的交迭区域里**——称为**解区、解空间**。
- N个训练模式将确定N个界面，每个界面都把权空间分为两个半空间，N个正的半子空间的交空间是以权空间原点为顶点的凸多面锥。满足上面各不等式的 $\vec w$ 必在该锥体中，即**锥中每一点都是上面不等式组的解，解矢量不是唯一的**，上述的**凸多面锥包含了解的全体**，称其为**解区、解空间或解锥**。
- 每一个训练模式都对解区提供一个约束，训练模式越多，解区的限制就越多，解区就越小。越靠近解区的中心，解矢量 $\vec w^*$ 就越可靠，由它构造的判别函数错分的可能性就越小。
- ![](images/3-4.png)

### 3.4.3 线性二分能力

- 线性判别函数的二分能力(Dichotomies)：是指线性函数对给定的N个n维二类模式的全部可能的类别分布情况，能正确分类的情况数，也称为线性二分能力。

- 若限定用线性判别函数，且样本在模式空间是**良好分布的**，即在n维模式空间中没有(n+1)个模式位于(n－1)维子空间中，可以证明：*N个n维二类模式集用线性判别函数能正确分类的方法总数*，称为：该模式集的**线性二分总数**，为：
  $$
  D(N,n)=
  \left\{
  \begin{aligned}
  & 2\sum^n_{j=0}C^j_{N-1}=2(C^0_{N-1}+C^1_{N-1}+...+C^n_{N-1})， & N>n+1 \\
  & 2^N=2(C^0_{N-1}+C^1_{N-1}+...+C^{N-1}_{N-1}), & N\le n+1
  \end{aligned}
  \right.
  $$
  或该模式集的**线性二分概率**为：
  $$
  P(N,n)=\frac{D(N,n)}{2^N}=
  \left\{
  \begin{aligned}
  & 2^{1-N}\sum^n_{j=0}C^j_{N-1}, &N>n+1 \\
  & 1, & N\le n+1
  \end{aligned}
  \right.
  $$




- **只要模式集中模式个数N小于或等于增广模式的维数（n+1），模式类总是线性可分的。**

- ​![3-4-2](images/3-4-2.png)该例子是n=1,N不断变换的情况，可见当 N=3>n+1时，线性可分总数不再是$2^N$。（该例子样本在模式空间良好分布，即没有2个样本在0维空间——同一点上。）

- 若定义$\lambda = N/(n+1)$，可观察到：

  - 在$\lambda \le 1$时，也即在$N\le (n+1)$时，有P(N,n)=1.

    即：这样的N个模式是线性可分的。

  - 在$1\le \lambda \le 2$时，也即$(n+1)\le N\le 2(n+1)$时，$0.5\le P(N,n)\le 1$

    即：当N固定、n增加时，模式集的线性二分概率是增加的。

  - 在$\lambda >2$时，也即在$N\ge (n+1)$时，$P(N,n)<0.5$

    即：当n固定、N增加时，模式集的线性二分概率迅速下降。

  将$\lambda=2$时的N值定义为阈值$N_0$，称为**二分法能力（容量）**，即：
  $$
  N_0=2(n+1)
  $$

- ![3-4-3](images/3-4-3.png)

## 3.6 感知器算法

- 用以利用已知类别的模式样本训练权向量W。

#### 3.6.1 感知器算法

$$
两类线性可分的模式类：\omega _1,\omega _2 ,设d(X)=W^TX, \\
其中，W=[w_1,w_2,w_3,...,w_4]^T,X=[x_1,x_2,...,x_n,1]^T \\
应具有性质\,\,d(X)=W^TX
\left\{
\begin{aligned}
& >0, &若X\in \omega_1 \\
& <0， &若X\in \omega_2
\end{aligned}
\right.
$$

- 规范化处理：即$\omega_2$类样本全部乘以-1，则有：
  $$
  d(X)=W^TX>0
  $$




- 感知器算法的基本思想：用训练模式验证当前权向量的合理性，如果不合理，就根据误差进行反向纠正，直到全部训练样本都被合理分类。本质上是**梯度下降方法类**。

- 算法步骤：

  - 选择N个分属于$w_1$和$w_2$类的模式样本构成训练样本集$\{X_1,...,X_N\}$

    构成增广向量形式，并进行规范化处理。任取权向量初始值W(1)，开始迭代。初始迭代次数k=1。

  - 用全部训练样本进行一轮迭代，计算$W^T(k)X_i$的值，并修正权向量。

    - 若$W^T(k)X_i\le 0$，说明分类器对第i个模式做了错误分类，校正为：
      $$
      W(k+1)=W(k)+cX_i
      $$

      - c：正的校正增量（步长）

    - 若$W^T(k)X_i>0$，分类正确，权向量不变：$W(k+1)=W(k)$

      统一写为：
      $$
      W(k+1)=
      \left\{
      \begin{aligned}
      &W(k),&若W^T(k)X_i>0 \\
      &W(k)+cX_i, &若W^T(k)X_i\le 0
      \end{aligned}
      \right.
      $$

  - 只要有一个错误分类，回到上一步，直到对所有样本正确分类。

- 当c、W(1)取其他值时，结果可能不一样，所以**感知器算法的解不唯一**。

- **感知器算法是一种赏罚过程：**

  - 分类正确时对权向量“赏”——不罚，即权向量不变;
  - 分类错误时对权向量“罚”——修改，向正确的方向转换。

#### 3.6.2 感知器算法的收敛性

- 收敛性：经过算法的有限次迭代运算后，求出了一个使所有样本都能正确分类的W，则称算法是收敛的。 
- 可以证明：感知器算法是收敛的。
- 收敛条件：模式类线性可分(即:存在一个权矢量与全部规范化增广矢量的内积大于等于零)。

#### 3.6.3 感知器算法用于多类情况

- 用多类情况1和多类情况2的方式分类时，**每个判别函数相当于一个两类问题的判别函数**，因此可以用感知器算法直接学习判别函数的权向量。而对多类情况3，感知器算法需要稍加修改才可以用来训练权向量。

- 对多类情况3，应有：

  > 若$X\in \omega_i，则d_i(X)>d_j(X), \forall j\ne i, j=1,...,M$， 对于M类模式应存在M个判决函数：{$d_i,i=1,...,M$}

- 算法主要内容：

  - 训练样本为增广向量模式，但==**不需要符号规范化处理**==。

  - 第k次迭代时，计算所有判别函数
    $$
    d_j(k)=W^T_j(k)X,j=1,...,M
    $$
    分两种情况修改权向量：

    1. 若$d_i(k)>d_j(k)，\forall j\ne i,j=1,2,...,M$，则权向量不变，$W_j(k+1)=W_j(k),j=1,2,...,M$

    2. 若第l个权向量使$d_i(k)\le d_l(k)$，则相应的权向量做调整：
       $$
       \left\{
       \begin{aligned}
       &W_i(k+1)=W_i(k)+cX \\
       &W_l(k+1)=W_l(k)-cX \\
       &W_j(k+1)=W_j(k),j\ne i,l
       \end{aligned}
       \right.
       ,c为正的校正增量
       $$

- 可以证明：只要模式类在情况3判别函数时是可分的，则经过有限次迭代后算法收敛。


## 3.7 梯度算法

### 3.7.1 梯度算法基本原理

#### 梯度概念

设函数f(Y)是向量$Y=[y_1,y_2,...,y_n]^T$的函数，则f(Y)的梯度定义为：
$$
\nabla f(Y)=\frac{d}{dY}f(Y)=[\frac{\partial f}{\partial y_1},\frac{\partial f}{\partial y_2},...,\frac{\partial f}{\partial y_n}]^T
$$

- 重要性质：**指出函数f在其自变量增加时，增长最快的方向**
  - 即：梯度的**方向**是函数$f(Y)$在Y点增长最快的方向
  - 梯度的**模**是$f(Y)$在增长最快的方向上的增长率（增长率最大值）
  - 所以**负梯度指出了最陡下降方向**

#### 梯度算法

设两个线性可分的模式类$w_i$和$w_2$的样本共N个，$w_2$类样本乘（-1），将两类样本分开的判决函数d(X)应满足：
$$
d(X_i)W^TX_i>0, i=1,2,...,N
$$
思想：**将联立不等式求解W的问题，转换成求准则函数极小值的问题。**

##### 基本思路

定义一个对错误分类敏感的准则函数$J(W,X)$，在J的梯度方向上对权向量进行修改，一般关系表示成从$W(k)$导出$W(k+1)$：
$$
W(k+1)=W(k)+c(-\nabla J)=W(k)-c\nabla J \\
W(k+1)=W(k)-c[\frac{\partial J(W,x)}{\partial W}]_{W=W(k)}
$$
其中c是正的比例因子。

##### 求解步骤

1. 规范化处理，选择准则函数，设置初始权向量W(1)，括号内为迭代次数k=1

2. 依次输入训练样本X，设第k次迭代时输入样本为$X_i$，此时以有权向量$W(k)$，求$\nabla J(k)$：
   $$
   \nabla J(k)=\frac{\partial J(W,X_i)}{\partial W}|_{W=W(k)}
   $$
   权向量修正为：
   $$
   W(k+1)=W(k)-c\nabla J(k)
   $$
   迭代次数加1，重复该步骤，直至对全部训练样本完成一轮迭代。

3. 在一轮迭代中，如果有一个样本使$\nabla J\ne 0$，会到2. 进行下一轮迭代，否则，W不再变化，算法收敛。

### 3.7.2 固定增量法

准则函数：**$J(W,X)=\frac{1}{2}(|W^TX|-W^TX)$**

当$W^TX>0$时，该准则函数有唯一最小值0

**固定增量算法：**
$$
\nabla J=\frac{\partial J(W,X)}{\part W}=\frac{1}{2}[Xsgn(W^TX)-X], \\
其中，sgn(W^TX)=\left\{
\begin{aligned}
& +1, &若W^TX>0 \\
&-1, &若W^TX\le 0
\end{aligned}
\right.
$$

$$
W(k+1)=W(k)+\left\{
\begin{aligned}
& 0,&若W^T(k)X>0 \\
& cX, &若W^T(k)X\le 0
\end{aligned}
\right.
$$

**感知器算法是梯度法的特例。**



## 3.8 最小平方误差算法（最小均方误差算法）

- LMS算法，亦称为Ho-Kashyap算法
- 感知器算法、梯度算法、固定增量算法或其他类似方法，只有当模式类可分离时才收敛，不可分情况下，算法会来回摆动，始终不收敛。
- LMS算法特点：
  - 对可分模式收敛
  - 对类别不可分的情况也能指出来

### 分类器的不等式方程

如果给出分属于$\omega_1,\omega_2$两个模式类的训练样本集$\{X_i,i=1,2,...,N\}$，应满足：$W^TX_i>0$，其中，$X_i$是规范化增广样本向量，$X_i=[x_{i1},x_{i2},...,x_{in},1]^T$

写成矩阵：
$$
\begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1n} & 1 \\
x_{21} & x_{22} & \cdots & x_{2n} & 1 \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
-x_{N1} & -x_{N2} & ... & x_{Nn} & 1 \\
\end{bmatrix}_{N*(n+1)}
\begin{bmatrix}
w_1 \\
w_2 \\
\vdots \\
w_n \\
w_{n+1}
\end{bmatrix}_{(n+1)*1}>
\begin{bmatrix}
0 \\
0 \\
\vdots \\
0 \\
0 \\
\end{bmatrix}_{N*1}=0
$$
即$XW>0$

### LMS算法

- 原理：把对满足$XW>0$的求解，改为满足$XW=B$的求解。

  ​	   其中B为各分量均为正值的矢量。

  - 因为一般训练样本数N总是大于模式的维数n，所以为矛盾方程组，无解，只能求近似解，即
    $$
    ||XW^*-B||=极小
    $$

  - **LMS出发点**：选择一个准则函数，使得当J达到最小值时，XW=B可得到近似解（**最小二乘近似解**）

- 准则函数定义为
  $$
  J(W,X,B)=\frac{1}{2}||XW-B||^2
  $$

  - 最小二乘：
    - 最小：使方程组两边误差最小，即J最小
    - 二乘：二次方
    - 即**最小平方（误差算法）**

- LMS算法核心：通过准则函数极小找W、B

- 算法递推公式：
  $$
  B(k+1)=B(k)+c[e(k)+|e(k)|] \\
  W(k+1)=W(k)+cX^\#|e(k)|\quad其中，X^\#=(X^TX)^{-1}X^T
  $$








#### 模式类别可分性判别

- **当模式集线性可分，且校正系数c满足$0<c\le 1$时，该算法收敛，可求得解W。**
- 通常每次迭代计算后都检查一下**XW(k)**和误差向量**e(k)**，从而可以判断是否收敛：

  分以下几种情况：
$$
  \begin{aligned}
  &①\ 如果e(k)=0，表明XW(k)=B(k)>0，有解。 \\
  &②\ 如果e(k)>0，表明XW(k)>B(k)>0，隐含有解，继续迭代，
  可使e(k)\rightarrow0.\\
  &③\ 如果e(k)<0（所有分量为负数或零，但不全为零），停止迭代，无解。
  \end{aligned}
$$
  综上所述：**只有当e(k)中有大于零的分量（说明：含有纠正信息）时，才需要继续迭代。**一旦e(k)的全部分量只有0和负数，则立即停止。

#### 算法描述

1. 根据N个分属于两类的样本，写出规范化增广样本矩阵X
2. 求X的伪逆矩阵$X^\#=(X^TX)^{-1}X^T$
3. 设置初值c和B(1)，c为正的校正增量，B(1)的各分量大于零，迭代次数k=1.开始迭代
4. 计算e(k)=XW(k)-B(k)，进行可分性判别。
5. 计算W(k+1)和B(k+1)，迭代次数加1，返回4.



## 3.9 非线性判别函数

- 用于线性不可分情况。如分段线性、超曲面。

### 3.9.1 分段线性判别函数

#### 一般分段线性判别函数

设有M类模式，将$\omega_i$类划分为$l_i$个子类：

$\omega_i:\{\omega_i^1,\omega_i^2,\omega_i^3,...,\omega_i^{l_i}\}\quad i=1,2,...,M$

其中第n个子类的判别函数：

$d_i^n(X)=(W_i^n)^TX \quad n=1,2,...,l_i;\ i=1,2,...,M$

$\omega_i$类的判别函数定义为：

$d_i(X)=max\{d_i^n(X),\quad n=1,2,...,l_i\}$

M类的判决规则：

$若d_j(X)=max\{d_i(X),\quad i=1,2,...,M\}，则X\in \omega_j$

#### 基于距离的分段线性判别函数

##### 最小距离分类器

$$
\begin{split}

&设\omega_1类均值向量：M_1=\frac{1}{N_1}\sum^{N_1}_{i=1}X_i \\
&w_2类均值向量：M_2=\frac{1}{N_2}\sum^{N_2}_{i=1}X_i
\\ \\ 
&任一模式X到M_1和M_2的欧氏距离平方：\\
&d_1(X)=||X-M_1||^2 \quad d_2(X)=||X-M_2||^2  \\
\\
&判决规则：若d_1(X)<d_2(X),则X\in \omega_1 \\
&若d_2(X)<d_1(X)，则X\in \omega_2 \\
&判别界面方程： \\
&||X-M_1||^2=||X-M_2||^2 \\
&化简得：2(M_1-M_2)^TX+(M_2^TM_2-M_1^TM_1)=0
\end{split}
$$

##### 分段线性距离分类器

$$
\begin{split}
& 设：M类模式，其中\omega_i类划分为l_i个子类，第n个子类的均值向量为M_i^n。每个子类的判别函数：\\
&\quad \quad d_i^n(X)=||X-M_i^n||^2,n=1,2,...,l_i;i=1,2,...,M \\
&每个类的判别函数：\\
&\quad\quad d_i(X)=min\{d_i^n(X),n=1,2,...,l_i\},i=1,2,...,M \\
&判决规则：\\
&\quad\quad若d_j(X)=min\{d_i(X),i=1,2,...,M\}，则X\in \omega_j
\end{split}
$$

### 3.9.2 分段线性判别函数的学习方法

1. 已知子类划分时的学习方法：
   - 每个子类看成独立的类
   - 在一类范围内根据多累情况3，学习各子类判别函数
   - 继而得到各类判别函数
2. 已知子类数目时的学习方法
   - 用类似于固定增量算法的错误修正算法学习分段线性判别函数。
3. 未知子类数目时的学习方法
   - 树状分段线性分类器

### 3.9.3 势函数法

#### 概念

- 每个点比拟为点能源，在点上势能达到峰值，随着与该点距离的增大，势能分布迅速减小。

  - $\omega_1$类样本势能为正——势能积累形成“高地”

  - $\omega_2$类样本势能*(-1)——势能积累形成“凹地”

    在两类电势分布之间，选择合适的等势面（如零等势面），即可认为是判别界面。

#### 势函数法判别函数的产生

设初始基类势函数$K_0(X)=0$，下标为迭代次数。

步骤：

1. 加入训练样本X1,
   $$
   K_1(X)=
   \left\{
   \begin{align}
   K_0(X)+K(X,X_1),若X_1\in \omega_1 \\
   K_0(X)-K(X,X_1),若X_1\in \omega_2
   \end{align}
   \right.
   $$
   $K_1(X)$描述了加入第一个样本后的边界划分。

2. 加第二个训练样本X2，分三种情况：
   $$
   \begin{align}
   (1)&若X_2\in\omega_1且K_1(X_2)>0,或X_2\in\omega_2且K_1(X_2)<0，\\
   &分类正确，势函数不变：K_2(X)=K_1(X) \\
   (2)&若X_2\in\omega_1但K_1(X_2)\le 0,错误分类，修改势函数：\\
   &K_2(X)=K_1(X)+K(X,X_2)=\pm K(X,X_1)+K(X,X_2) \\
   (3)&若X_2\in\omega_2但K_1(X_2)\ge0，错误分类，修改势函数： \\
   &K_2(X)=K_1(X)-K(X,X_2)=\pm K(X,X_1)-K(X,X_2)
   \end{align}
   $$
   .......

   以上决定积累位势的迭代算法可写成：
   $$
   K_{k+1}(X)=K_k(X)+r_{k+1}K(X,X_{k+1}) 
   $$
   其中$r_{k+1}$为校正项系数，定义为：
   $$
   r_{k+1}=
   \left\{
   \begin{align}
   0,\quad &对于X_{k+1}\in\omega_1且K_k(X_{k+1})>0 \\
   0,\quad &对于X_{k+1}\in\omega_2且K_k(X_{k+1})<0 \\
   1,\quad &对于X_{k+1}\in\omega_1且K_k(X_{k+1})\le0 \\
   -1,\quad &对于X_{k+1}\in\omega_2且K_k(X_{k+1})\ge0 \\
   \end{align}
   \right.
   $$
   略去不使积累势发生变化的样本，可得一简化样本序列$\{\widehat{X_1},\widehat{X_2},...,\widehat{X_j},...\}$，算法可归纳为：
   $$
   K_{k+1}(X)=\sum_{X_j}\alpha_jK(X,\widehat{X_j}),式中 \\
   \alpha_j=\left\{
   \begin{align}
   1,\quad &对于\widehat{X_j}\in\omega_1 \\
   -1,\quad&对于\widehat{X_j}\in\omega_2 
   \end{align}
   \right.
   $$
   即：由(k+1)个训练样本产生的积累势，等于两类中校正错误的样本的总势能之差。



#### 势函数的选择

- 势函数应具备的条件：

  两个n维向量$X$和$X_k$的函数$K(X,X_k)$，如同时满足下列三个条件，都可作为势函数：

  - $K(X,X_k)=K(X_k,X)$，当且仅当$X=X_k$时达到最大值
  - 当向量$X$与$X_k$的距离趋于无穷时，$K(X,X_k)$时趋于零
  - $K(X,X_k)$是光滑函数，且是$X$与$X_k$之间距离的单调下降函数

- 构成势函数的方法：

  - **Ⅰ型势函数**：用对称的有限项多项式展开，即：
    $$
    K(X,X_k)=\sum^m_{i=1}\varphi_i(X_k)\varphi_i(X)
    $$
    式中$\{ \varphi_i (X),i=1,2,...\}$，在模式定义域内应为正交函数集（即内积=0，内积：$(y,z)=\int^b_ay(x)z(x)d(x)$）

    判别函数：
    $$
    d_{k+1}(X)=d_k(X)+\sum^m_{i=1}r_{k+1}\varphi_i(X_{k+1})\varphi_i(X)
    $$

  - **Ⅱ型势函数**：直接选择双变量$X$和$X_k$对称函数作为势函数，即$K(X,X_k)=K(X_k,X)$ 

