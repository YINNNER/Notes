

# Business Intelligence 商务智能

## I. Descriptive Analytics 描述性分析

## II. Predictive Analytics 预测性分析

### 5. Classification 分类过程和方法

### 6. Text Analytics and Text Mining 文本与网络挖掘

- Text Analytics versus Text Mining
- Text Analytics =
  - Information Retrieval + 信息检索
  - Information Extraction + 信息提取
  - Data Mining + 数据挖掘
  - Web Mining 网页挖掘
- or simply
- Text Analytics = Information Retrieval + Text Mining 信息检索+文本挖掘

#### Text Mining Concepts

- 85-90 percent of 
- text mining:
  - A semi-automated 

#### Data Mining Versus Text Mining

- Both seek for novel and useful patterns
- Both are semi-automated processes 半自动过程
- Difference is the nature of the data:
  - Structured versus unstructured data
  - **Structured data**: in databases     ==> Data Mining
  - **Unstructured data**: Word documents, PDF files, text excerpts, XML files, and so on.   ==> Text Mining
- process: impose structure to the data, mine the structured data.

#### Text Mining Terminology

- Unstructured data or semistructured data
- Corpus (and corpora) 语料库
- Terms 词项
- Concepts 概念
- Stemming 词干
- Stop words 停用词(在分析语境中无意义的词)  (and include words) 
- Synonyms  同义词(and polysemes)
- Tokenizing

- Term dictionary 词汇表，特别用途的词

- word frequency 词频

- Part-of-speech tagging 词性标记

- Morphology 词法、形态学

- Term-by-document matrix 术语文档矩阵：某些词出现不同文档中，可将这些信息量放在一个矩阵中。

  > A **document-term matrix** or **term-document matrix** is a mathematical [matrix](https://en.wikipedia.org/wiki/Matrix_(mathematics)) that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms. There are various schemes for determining the value that each entry in the matrix should take. One such scheme is [tf-idf](https://en.wikipedia.org/wiki/Tf-idf). They are useful in the field of [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing).
  >
  > 文档术语矩阵或术语 - 文档矩阵是描述文档集合中出现的术语频率的数学矩阵。在文档术语矩阵中，行对应于集合中的文档，列对应于术语。有各种方案可用于确定矩阵中每个条目应采用的值。一个这样的方案是tf-idf。它们在自然语言处理领域很有用。

  - Occurrence matrix 两个词出现在一个句子中的数目

- Singular value decomposition

  - latent semantic indexing

#### Natural Language Processing (NLP) 自然语言处理

- Structuring a collection of text
  - Old approach: bag-of-words
  - New approach: natural language processing
- NLP is
  - a very important concept in text mining
  - a subfield of artificial intelligence ...
  - .....
- WordNet
- Sentiment Analysis 情感分析
  - A technique used to detect favorable and unfavorable opinions toward specific products and services
  - SentiWordNet

#### Sentiment Analysis 情感分析

- Sentiment  -->  belief, view, opinion, and conviction
- Sentiment analysis is trying to answer the question "What do people feel about a certain topic?"
- By analyzing data related to opinions of many using a variety of automated tools
- Used in variety ....
- P-N Polarity: P-positive; N-negative
- S-O Polarity: S-subjective; O-objective

#### Web Mining

- Web mining is the process of discovering instrinsic relationships from Web data (textual, linkage, usage)

#### Web Content/Struture Mining

- Mining the textual content on the Web
- Data collection via web crawlers

### 7. Clustering Analysis 聚类分析

#### 7.1 Define 定义

- Cluster analysis or simply clustering is the process of partitioning a set of data objects into subsets. 
- Each subset is a **cluster簇**, such that objects in a cluster are similar to one another, yet dissimilar to objects in other clusters. 

High cohesion and low coupling  高内聚，低耦合

####  7.2 Classification and Clustering 区分

- **Classification** is known as **supervised learning 有监督学习** because the class label information is given. It generates a classification model through which we can predict the class of a sample. 

- **Clustering** is known as **unsupervised learning 无监督学习** because the class label information is not present. By analyzing corresponding attributes, clustering place samples with similar attributes into a class. 

#### 7.3 Partitioning Method 划分方法

- Given a set D of n objects, a partitioning method constructs k partitions of the data, where each partition represents a cluster and k ≤ n . 

- Traditional partitioning methods are often computationally prohibitive, potentially requiring an exhaustive *enumeration of all the possible partitions.* 
- Instead, most applications adopt popular **heuristic methods 启发式方法** such as: The **k‐means(k-均值)** and the **k‐medoids(k-中心点)** algorithms. 

#### 7.4 K‐Means:A Centroid‐Based Technique k-均值算法

##### 7.4.1 Basic

K‐均值：一种基于形心的技术

- A centroid‐based technique: uses the **centroid形心** $c_i$ of a cluster $C_i$ to represent that cluster. 

- The difference between an object $p∈ C_i$ and $c_i$ is measured by$ dist(p,c_i)$, the quality of cluster $C_i$ can be measured by the **within‐cluster variation簇内变差**, which is the sum of squared error between all objects in $C_i$ and the centroid $c_i$ , defined as : 
  $$
  E = \sum^k_{i=1}\sum_{p\in C_i}(d(p,c_i))^2
  $$


- Optimizing the within‐cluster variation is computationally challenging and we may have to enumerate a number of possible partitions. Thus, the k‐means algorithm is used in practice. 
- 划分采用互斥划分。

##### 7.4.2 K‐means partitioning algorithm

- Input: 

  k: the number of clusters,
  D:a data set containing n objects. 

- Output: A set of k clusters.

1. arbitrarily choose k objects from D as the initial cluster centers and calculate cluster means; 

2. repeat 

3. assign each object to the cluster to which the object is the most similar, based o the mean value of the objects in the cluster; 

4. Update the cluster means, that is , calculate the mean value of the objects for each cluster. 

5. until no change; 

> K‐means algorithm defines the centroid of a cluster as the mean value of the points within the cluster.
>
> K‐均值算法把簇的形心定义为簇均值，即簇内对象的均值。

##### 7.4.3 Strength & Weakness

- Strength：
  - The idea is simple and feasible.
  - The time complexity is close to linear.
  - For large data sets, it is scalable and effective. 

- Weakness：
  - Need to specify k, the number of clusters, in advance and the result may not be well with inappropriate value of k .  要求用户事先给出要生成的簇数k，不适合的k值可能返回较差的结果。 
  - Not suitable to discover clusters with non‐convex shapes or clusters of very different size. 不适合发现非凸形状的簇，或者大小差别很大的簇。
  - Sensitive to noisy data and outliers. 对噪声和离群点敏感。

#### 7.5 K‐Medoids: A Representative Object‐Based Technique k-中心点算法

##### 7.5.1 Basic

K‐中心点：一种基于代表对象的技术

- The k‐means algorithm is sensitive to outliers because such objects are far away from the majority of the data, and thus, when assigned to a cluster, they can dramatically distort the mean value of the cluster. This effect is particularly deteriorate due to the use of the squared‐error function.

  K均值算法对于离群点是敏感的，因为一个有很大极端值的对象可能显著地扭曲数据的分布。平方差的使用更是严重恶化了这一影响。**(离群点对形心/簇内均值的位置影响很大)** 

- Instead, we **pick actual objects to represent the clusters**. 在每个簇中选出一个实际的对象来代 簇，而不是利用均值。And, **an absolute‐error criterion绝对误差标准** is used: 
  $$
  E = \sum^k_{i=1}\sum_{p\in C_i}(d(p,o_i))^2
  $$

- The k‐medoids method groups n objects into k clusters by minimizing the absolute error. 

##### 7.5.2 Process of the algorithm

实现过程: 

- 设􏰀􏰁 $o_1$,...,$o_k$􏰀􏰂 是当前代表对象(即中心点)的集合。为了决定一个非代表对象 $o_{random}$ 􏰀􏰃􏰄􏰅􏰆􏰇􏰈是否是一个当前中心点 $o_j (1 \le j \le k)$  的好的替代，我们计算每个对象p到集合$\{o_1􏰀􏰁,...,o_{j-1}􏰀􏰍􏰎􏰁,􏰀􏰃􏰄􏰅􏰆􏰇􏰈o_{random},o_{j+1} 􏰀􏰉􏰏􏰁,...,o_k􏰀􏰂\}$中最近对象的距离，并使用该距离更新**代价函数**$= d(o_i)-d(o_j)$ (重新分配给的$o_i -$之前分配给的 $o_j$)。 

四种情况：**总之就是离谁近就分配给谁**

1. p当前隶属于 $o_j$ 􏰀􏰉，如果 $o_j$ 􏰀􏰉􏰀被􏰄􏰅􏰆􏰇􏰈 $o_{random}$ 􏰀􏰉所代替作为代表对象，且p离其他代表对象 $o_i(i\ne j)$ 最近，则p重新分配给 $o_i$ 􏰀􏰉􏰀􏰐。

2. p当前隶属于􏰀􏰉 $o_j$，如果 $o_j$ 􏰀􏰉􏰀被􏰄􏰅􏰆􏰇􏰈 $o_{random}$ 􏰀􏰉所代替作为代表对象，且p离􏰀􏰃􏰄􏰅􏰆􏰇􏰈 􏰄􏰅􏰆􏰇􏰈 $o_{random}$ 最近，则p重新分配给􏰀􏰃􏰄􏰅􏰆􏰇􏰈􏰄􏰅􏰆􏰇􏰈 $o_{random}$。

3. p当前隶属于 $o_i(i\ne j)$ 􏰋，如果 $o_j$ 􏰀􏰉􏰀被􏰄􏰅􏰆􏰇􏰈 $o_{random}$ 􏰀􏰉所代替作为代表对象，而p依然离􏰀􏰐 􏰄􏰅􏰆􏰇􏰈 $o_i$ 最近，则对象的隶属不变。 

4. p当前隶属于 $o_i(i\ne j)$ 􏰋，如果 $o_j$ 􏰀􏰉􏰀被􏰄􏰅􏰆􏰇􏰈 $o_{random}$ 􏰀􏰉所代替作为代表对象，而p依然离􏰀􏰃􏰄􏰅􏰆􏰇􏰈􏰄􏰅􏰆􏰇􏰈 $o_{random}$最近，则p重新分配给􏰀􏰃􏰄􏰅􏰆􏰇􏰈􏰄􏰅􏰆􏰇􏰈 $o_{random}$。 

- 每当重新分配发生时，绝对误差E的差对代价函数影响。因此，如果当前的代表对象被非代表对象所取代，就计算绝对误差值的差。交换的总代价是所有非代表对象所产生的代价之和。

- 如果总代价是负的，实际绝对误差将会减小， $o_j$ 􏰀􏰉􏰀􏰉可以 􏰀􏰃􏰄􏰅􏰆􏰇􏰈􏰄􏰅􏰆􏰇􏰈 $o_{random}$ 􏰀􏰉替代。
- 如果总代价是正的，则当前的代表对象􏰀􏰉是可接受的，在本次迭代中没有变化发生。 

##### 7.5.3 Algorithm

![k_medoids](images/k_medoids.png)

#### Hierarchical Methods 层次方法

层次聚类方法(hierarchical clustering method)将数据对象组成一棵聚类树。

- Hierarchical clustering methods work by grouping data objects into a hierarchy or tree of clusters.
- A hierarchical method can be either agglomerative or divisive, depending on whether the hierarchical decomposition is formed in a bottom‐up(merging) or top‐down (splitting) fashion. 

- The quality of a pure hierarchical clustering methodis limited by the fact that once merged or split, execution can not be corrected. That is to say, if a merger or split decision is later proved to be a bad choice, the method can not be returned and corrected. 

- 根据层次分解是以自底向上(合并)还是自顶向下 (分裂)方式，层次聚类方法可以进一步分为凝聚的和分裂的。 

- 一种纯粹的层次聚类方法的质量受限于：一旦合并或分裂执行，就不能修正。也就是说，如果某个合 并或分裂决策在后来证明是不好的选择，该方法无法退回并更正。

==（详见模式识别）==

